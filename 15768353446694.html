<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  TensorFlow Sparse的一个优化 - 
  
  </title>
 <meta name="description" content="">
 <link href="atom.xml" rel="alternate" title="" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />

    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
    <script src="asset/highlightjs/highlight.pack.js"></script>
    <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
    <script>hljs.initHighlightingOnLoad();</script>
    
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>

<div id="header">
    <h1><a href="index.html"></a></h1>
</div>

</nav>
        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; </span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
      <li><a href="index.html">Home</a></li>
      
        <li class="divider"></li>
        <li><label>推荐系统</label></li>

          
            <li><a title="推荐系统的禅与道" href="15951226773958.html">推荐系统的禅与道</a></li>
          
            <li><a title="推荐系统灌水文之如何理解Item" href="15763400595374.html">推荐系统灌水文之如何理解Item</a></li>
          

      
        <li class="divider"></li>
        <li><label>TensorFlow</label></li>

          
            <li><a title="TensorFlow Sparse的一个优化" href="15768353446694.html">TensorFlow Sparse的一个优化</a></li>
          
            <li><a title="基础知识背景" href="15768070172080.html">基础知识背景</a></li>
          
            <li><a title="TensorFlow优化" href="15744799610685.html">TensorFlow优化</a></li>
          
            <li><a title="GDG上海 2019.10.13分享内容准备" href="15673444950441.html">GDG上海 2019.10.13分享内容准备</a></li>
          
            <li><a title="Dive Into TensorFlow" href="15659603996762.html">Dive Into TensorFlow</a></li>
          
            <li><a title="Maybe Best Practice With Sparse Machine Learning In TensorFlow" href="15430699092916.html">Maybe Best Practice With Sparse Machine Learning In TensorFlow</a></li>
          

      
        <li class="divider"></li>
        <li><label>机器学习平台</label></li>

          
            <li><a title="---" href="16533711069508.html">---</a></li>
          
            <li><a title="机器学习平台在云音乐的持续实践" href="16371350579612.html">机器学习平台在云音乐的持续实践</a></li>
          
            <li><a title="泛模型管理与分发" href="16076083666793.html">泛模型管理与分发</a></li>
          
            <li><a title="机器学习与容器化平台" href="15861773530303.html">机器学习与容器化平台</a></li>
          
            <li><a title="机器学习工程实践" href="15648106167930.html">机器学习工程实践</a></li>
          

      
        <li class="divider"></li>
        <li><label>图计算</label></li>

          
            <li><a title="直播场景PGL落地" href="16078481811620.html">直播场景PGL落地</a></li>
          
            <li><a title="Graph Neural Networks" href="16077533959046.html">Graph Neural Networks</a></li>
          
            <li><a title="graph representation learning" href="16077533959085.html">graph representation learning</a></li>
          
            <li><a title="Spectral Clustering" href="16029890553575.html">Spectral Clustering</a></li>
          
            <li><a title="Community structure in networks" href="16019601035128.html">Community structure in networks</a></li>
          
            <li><a title="Motifs and structural roles in networks" href="16017096445579.html">Motifs and structural roles in networks</a></li>
          
            <li><a title="Properties of Networks and Random Graph Model" href="16016274752497.html">Properties of Networks and Random Graph Model</a></li>
          

      
        <li class="divider"></li>
        <li><label>pytorch</label></li>

          
            <li><a title="pytorch 环境安装及配置" href="15808187045354.html">pytorch 环境安装及配置</a></li>
          

      
        <li class="divider"></li>
        <li><label>分布式系统</label></li>

          
            <li><a title="分布式一致性" href="15860748439491.html">分布式一致性</a></li>
          
            <li><a title="raft 参考文章" href="15854050053037.html">raft 参考文章</a></li>
          
            <li><a title="分布式系统书籍" href="15853877486542.html">分布式系统书籍</a></li>
          
            <li><a title="容灾【备份】" href="15848127872361.html">容灾【备份】</a></li>
          
            <li><a title="容灾【备份】" href="15848127093419.html">容灾【备份】</a></li>
          

      
        <li class="divider"></li>
        <li><label>mlops</label></li>

          
            <li><a title="模型生产环境中的反馈与数据回流" href="16490569390259.html">模型生产环境中的反馈与数据回流</a></li>
          
            <li><a title="mlops之监控与数据回流" href="16482627913984.html">mlops之监控与数据回流</a></li>
          
            <li><a title="为机器学习量身定做的ops工具：cml & dvc" href="16268581441013.html">为机器学习量身定做的ops工具：cml & dvc</a></li>
          
            <li><a title="[toc]" href="16003962131575.html">[toc]</a></li>
          

      
        <li class="divider"></li>
        <li><label>paddle</label></li>

          
            <li><a title="直播场景PGL落地" href="16078655202819.html">直播场景PGL落地</a></li>
          

      
        <li class="divider"></li>
        <li><label>参数服务器</label></li>

          
            <li><a title="ps-lite" href="15836784836219.html">ps-lite</a></li>
          
            <li><a title="ps-lite再一次研读源码" href="15536014868367.html">ps-lite再一次研读源码</a></li>
          

      
        <li class="divider"></li>
        <li><label>GAN</label></li>

          
            <li><a title="深度解析为什么GAN能是Structured Learning的一种解决方案" href="Introduction%20to%20GAN.html">深度解析为什么GAN能是Structured Learning的一种解决方案</a></li>
          

      
      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>

        <section id="main-content" role="main" class="scroll-container">

          <div class="row">
            <div class="large-3 medium-3 columns">
              <div class="hide-for-small">
                <div class="sidebar">
                <nav>
                  <ul id="side-nav" class="side-nav">

                    
                      <li class="side-title"><span>推荐系统</span></li>
                        
                          <li><a title="推荐系统的禅与道" href="15951226773958.html">推荐系统的禅与道</a></li>
                        
                          <li><a title="推荐系统灌水文之如何理解Item" href="15763400595374.html">推荐系统灌水文之如何理解Item</a></li>
                        

                    
                      <li class="side-title"><span>TensorFlow</span></li>
                        
                          <li><a title="TensorFlow Sparse的一个优化" href="15768353446694.html">TensorFlow Sparse的一个优化</a></li>
                        
                          <li><a title="基础知识背景" href="15768070172080.html">基础知识背景</a></li>
                        
                          <li><a title="TensorFlow优化" href="15744799610685.html">TensorFlow优化</a></li>
                        
                          <li><a title="GDG上海 2019.10.13分享内容准备" href="15673444950441.html">GDG上海 2019.10.13分享内容准备</a></li>
                        
                          <li><a title="Dive Into TensorFlow" href="15659603996762.html">Dive Into TensorFlow</a></li>
                        
                          <li><a title="Maybe Best Practice With Sparse Machine Learning In TensorFlow" href="15430699092916.html">Maybe Best Practice With Sparse Machine Learning In TensorFlow</a></li>
                        

                    
                      <li class="side-title"><span>机器学习平台</span></li>
                        
                          <li><a title="---" href="16533711069508.html">---</a></li>
                        
                          <li><a title="机器学习平台在云音乐的持续实践" href="16371350579612.html">机器学习平台在云音乐的持续实践</a></li>
                        
                          <li><a title="泛模型管理与分发" href="16076083666793.html">泛模型管理与分发</a></li>
                        
                          <li><a title="机器学习与容器化平台" href="15861773530303.html">机器学习与容器化平台</a></li>
                        
                          <li><a title="机器学习工程实践" href="15648106167930.html">机器学习工程实践</a></li>
                        

                    
                      <li class="side-title"><span>图计算</span></li>
                        
                          <li><a title="直播场景PGL落地" href="16078481811620.html">直播场景PGL落地</a></li>
                        
                          <li><a title="Graph Neural Networks" href="16077533959046.html">Graph Neural Networks</a></li>
                        
                          <li><a title="graph representation learning" href="16077533959085.html">graph representation learning</a></li>
                        
                          <li><a title="Spectral Clustering" href="16029890553575.html">Spectral Clustering</a></li>
                        
                          <li><a title="Community structure in networks" href="16019601035128.html">Community structure in networks</a></li>
                        
                          <li><a title="Motifs and structural roles in networks" href="16017096445579.html">Motifs and structural roles in networks</a></li>
                        
                          <li><a title="Properties of Networks and Random Graph Model" href="16016274752497.html">Properties of Networks and Random Graph Model</a></li>
                        

                    
                      <li class="side-title"><span>pytorch</span></li>
                        
                          <li><a title="pytorch 环境安装及配置" href="15808187045354.html">pytorch 环境安装及配置</a></li>
                        

                    
                      <li class="side-title"><span>分布式系统</span></li>
                        
                          <li><a title="分布式一致性" href="15860748439491.html">分布式一致性</a></li>
                        
                          <li><a title="raft 参考文章" href="15854050053037.html">raft 参考文章</a></li>
                        
                          <li><a title="分布式系统书籍" href="15853877486542.html">分布式系统书籍</a></li>
                        
                          <li><a title="容灾【备份】" href="15848127872361.html">容灾【备份】</a></li>
                        
                          <li><a title="容灾【备份】" href="15848127093419.html">容灾【备份】</a></li>
                        

                    
                      <li class="side-title"><span>mlops</span></li>
                        
                          <li><a title="模型生产环境中的反馈与数据回流" href="16490569390259.html">模型生产环境中的反馈与数据回流</a></li>
                        
                          <li><a title="mlops之监控与数据回流" href="16482627913984.html">mlops之监控与数据回流</a></li>
                        
                          <li><a title="为机器学习量身定做的ops工具：cml & dvc" href="16268581441013.html">为机器学习量身定做的ops工具：cml & dvc</a></li>
                        
                          <li><a title="[toc]" href="16003962131575.html">[toc]</a></li>
                        

                    
                      <li class="side-title"><span>paddle</span></li>
                        
                          <li><a title="直播场景PGL落地" href="16078655202819.html">直播场景PGL落地</a></li>
                        

                    
                      <li class="side-title"><span>参数服务器</span></li>
                        
                          <li><a title="ps-lite" href="15836784836219.html">ps-lite</a></li>
                        
                          <li><a title="ps-lite再一次研读源码" href="15536014868367.html">ps-lite再一次研读源码</a></li>
                        

                    
                      <li class="side-title"><span>GAN</span></li>
                        
                          <li><a title="深度解析为什么GAN能是Structured Learning的一种解决方案" href="Introduction%20to%20GAN.html">深度解析为什么GAN能是Structured Learning的一种解决方案</a></li>
                        

                    
                  </ul>
                </nav>
                </div>
              </div>
            </div>
            <div class="large-9 medium-9 columns">

 <div class="markdown-body">
<h1>TensorFlow Sparse的一个优化</h1>

<p>分布式TensorFlow在Sparse模型上的实践<br />
前言<br />
如果你去搜TensorFlow教程，或者分布式TensorFlow教程，可能会搜到很多mnist或者word2vec的模型，但是对于Sparse模型(LR, WDL等)，研究的比较少，对于分布式训练的研究就更少了。最近刚刚基于分布式TensorFlow上线了一个LR的CTR模型，支持百亿级特征，训练速度也还不错，故写下一些个人的心得体会，如果有同学也在搞类似的模型欢迎一起讨论。</p>
<p>训练相关<br />
集群setup<br />
分布式TensorFlow中有三种角色: Master(Chief), PS和Worker. 其中Master负责训练的启动和停止，全局变量的初始化等；PS负责存储和更新所有训练的参数(变量); Worker负责每个mini batch的梯度计算。如果去网上找分布式TensorFlow的教程，一个普遍的做法是使用Worker 0作为Master, 例如这里官方给出的例子。我个人比较建议把master独立出来，不参与训练，只进行训练过程的管理, 这样有几个好处:</p>
<p>独立出Master用于训练的任务分配，状态管理，Evaluation, Checkpoint等，可以让代码更加清晰。<br />
Master由于任务比worker多，往往消耗的资源如CPU/内存也和worker不一样。独立出来以后，在k8s上比较容易分配不同的资源。<br />
一些训练框架如Kubeflow使用Master的状态来确定整个任务的状态。<br />
训练样本的分配<br />
在sparse模型中，采用分布式训练的原因一般有两种:</p>
<p>数据很多，希望用多个worker加速。<br />
模型很大，希望用多个PS来加速。<br />
前一个就涉及到训练样本怎么分配的问题。从目前网上公开的关于分布式TF训练的讨论，以及官方分布式的文档和教程中，很少涉及这个问题，因此最佳实践并不清楚。我目前的解决方法是引入一个zookeeper进行辅助。由master把任务写到zookeeper, 每个worker去读取自己应该训练哪些文件。基本流程如下:</p>
<h1><a id="master" class="anchor" aria-hidden="true" href="#master"><span class="octicon octicon-link"></span></a>master</h1>
<p>file_list = list_dir(flags.input_dir)<br />
for worker, worker_file in zip(workers, file_list):<br />
self.zk.create(f'/jobs/{worker}', ','.join(worker_file).encode('utf8'), makepath=True)</p>
<h1><a id="start-master-session" class="anchor" aria-hidden="true" href="#start-master-session"><span class="octicon octicon-link"></span></a>start master session ...</h1>
<h1><a id="worker" class="anchor" aria-hidden="true" href="#worker"><span class="octicon octicon-link"></span></a>worker</h1>
<p>wait_for_jobs = Semaphore(0)<br />
@self.zk.DataWatch(f&quot;/jobs/{worker}&quot;)<br />
def watch_jobs(jobs, _stat, _event):<br />
if jobs:<br />
data_paths = jobs.decode('utf8').split(',')<br />
self.train_data_paths = data_paths<br />
wait_for_jobs.release()</p>
<p>wait_for_jobs.acquire()</p>
<h1><a id="start-worker-session" class="anchor" aria-hidden="true" href="#start-worker-session"><span class="octicon octicon-link"></span></a>start worker session ...</h1>
<p>引入zookeeper之后，文件的分配就变得非常灵活，并且后续也可以加入使用master监控worker状态等功能。</p>
<p>使用device_filter<br />
启动分布式TF集群时，每个节点都会启动一个Server. 默认情况下，每个节点都会跟其他节点进行通信，然后再开始创建Session. 在集群节点多时，会带来两个问题：</p>
<p>由于每个节点两两通信，造成训练的启动会比较慢。<br />
当某些worker挂掉重启（例如因为内存消耗过多），如果其他某些worker已经跑完结束了，重启后的worker就会卡住，一直等待其他的worker。此时会显示log: CreateSession still waiting for response from worker: /job:worker/replica:0/task:x.<br />
解决这个问题的方法是使用device filter. 例如:</p>
<p>config = tf.ConfigProto(device_filters=[&quot;/job:ps&quot;, f&quot;/job:{self.task_type}/task:{self.task_index}&quot;])</p>
<p>with tf.train.MonitoredTrainingSession(<br />
...,<br />
config=config<br />
):<br />
...<br />
这样，每个worker在启动时就只会和所有的PS进行通信，而不会和其他worker进行通信，既提高了启动速度，又提高了健壮性。</p>
<p>给session.run加上timeout<br />
有时在一个batch比较大时，由于HDFS暂时不可用或者延时高等原因，有可能会导致session.run卡住。可以给它加一个timeout来提高程序的健壮性。</p>
<p>session.run(fetches, feed_dict, options=tf.RunOptions(timeout_in_ms=timeout))<br />
优雅的停止训练<br />
在很多分布式的例子里(例如官方的这个)，都没有涉及到训练怎么停止的问题。通常的做法都是这样:</p>
<p>server = tf.train.Server(<br />
cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)<br />
if FLAGS.job_name == &quot;ps&quot;:<br />
server.join()<br />
这样的话，一旦开始训练，PS就会block, 直到训练结束也不会停止，只能暴力删除任务(顺便说一下，MXNet更粗暴，PS运行完import mxnet就block了)。前面我们已经引入了zookeeper, 所以可以相对比较优雅的解决这个问题。master监控worker的状态，当worker全部完成后，master设置自己的状态为完成。PS和worker检测到master完成后，就结束自己。</p>
<p>def join():<br />
while True:<br />
status = self.zk.get_async('/status/master').get(timeout=3)<br />
if status == 'done':<br />
break<br />
time.sleep(10)</p>
<h1><a id="master" class="anchor" aria-hidden="true" href="#master"><span class="octicon octicon-link"></span></a>master</h1>
<p>for worker in workers:<br />
status = self.zk.get_async(f'/status/worker/{worker}').get(timeout=3)<br />
update_all_workers_done()<br />
if all_workers_done:<br />
self.zk.create('/status/master', b'done', makepath=True)</p>
<h1><a id="ps" class="anchor" aria-hidden="true" href="#ps"><span class="octicon octicon-link"></span></a>ps</h1>
<p>join()</p>
<h1><a id="worker" class="anchor" aria-hidden="true" href="#worker"><span class="octicon octicon-link"></span></a>worker</h1>
<p>while has_more_data:<br />
train()<br />
self.zk.set('/status/worker/{worker}', b'done')<br />
join()<br />
Kubeflow的使用<br />
Kubeflow是一个用于分布式TF训练提交任务的项目。它主要由两个部分组成，一是ksonnet，一是tf-operator。ksonnet主要用于方便编写k8s上的manifest, tf-operator是真正起作用的部分，会帮你设置好TF_CONFIG这个关键的环境变量，你只需要设置多少个PS, 多少个worker就可以了。个人建议用ksonnet生成一个skeleton, 把tf-operator的manifest打印出来(使用ks show -c default), 以后就可以跳过ksonnet直接用tf-operator了。因为k8s的manifest往往也需要修改很多东西，最后还是要用程序生成，没必要先生成ksonnet再生成yaml.</p>
<p>Estimator的局限性<br />
一开始我使用的是TF的高级API Estimator, 参考这里的WDL模型. 后来发现Estimator有一些问题，没法满足目前的需要。</p>
<p>一是性能问题。我发现同样的linear model, 手写的模型是Estimator的性能的5-10倍。具体原因尚不清楚，有待进一步调查。</p>
<p>UPDATE: 性能问题的原因找到了，TL;DR: 在sparse场景使用Dataset API时, 先batch再map(parse_example)会更快。详情见: <a href="https://stackoverflow.com/questions/50781373/using-feed-dict-is-more-than-5x-faster-than-using-dataset-api">https://stackoverflow.com/questions/50781373/using-feed-dict-is-more-than-5x-faster-than-using-dataset-api</a></p>
<p>二灵活性问题。Estimator API目前支持分布式的接口是train_and_evaluate, 它的逻辑可以从源代码里看到:</p>
<h1><a id="training-py" class="anchor" aria-hidden="true" href="#training-py"><span class="octicon octicon-link"></span></a>training.py</h1>
<p>def run_master(self):<br />
...<br />
self._start_distributed_training(saving_listeners=saving_listeners)</p>
<pre><code>if not evaluator.is_final_export_triggered:
    logging.info('Training has already ended. But the last eval is skipped '
                'due to eval throttle_secs. Now evaluating the final '
                'checkpoint.')
    evaluator.evaluate_and_export()
</code></pre>
<h1><a id="estimator-py" class="anchor" aria-hidden="true" href="#estimator-py"><span class="octicon octicon-link"></span></a>estimator.py</h1>
<p>def export_savedmodel(...):<br />
...<br />
with tf_session.Session(config=self._session_config) as session:<br />
saver_for_restore = estimator_spec.scaffold.saver or saver.Saver(<br />
sharded=True)<br />
saver_for_restore.restore(session, checkpoint_path)<br />
...<br />
builder = ...<br />
builder.save(as_text)<br />
可以看到Estimator有两个问题，一是运行完才进行evaluate, 但在大型的sparse model里，训练一轮可能要几个小时，我们希望一边训练一边evaluate, 在训练过程中就能尽快的掌握效果的变化(例如master只拿到test set, worker只拿到training set, 这样一开始就能evaluate)。二是每次导出模型时，Estimator API会新建一个session, 加载最近的一个checkpoint, 然后再导出模型。但实际上，一个sparse模型在TF里耗的内存可能是100多G，double一下会非常浪费，而且导出checkpoint也极慢，也是需要避免的。基于这两个问题，我暂时没有使用Estimator来进行训练。</p>
<p>使用feature column(但不使用Estimator)<br />
虽然不能用Estimator, 但是使用feature column还是完全没有问题的。不幸的是网上几乎没有人这么用，所以我把我的使用方法分享一下，这其实也是Estimator的源代码里的使用方式:</p>
<h1><a id="define-feature-columns-as-usual" class="anchor" aria-hidden="true" href="#define-feature-columns-as-usual"><span class="octicon octicon-link"></span></a>define feature columns as usual</h1>
<p>feature_columns = ...<br />
example = TFRecordDataset(...).make_one_shot_iterator().get_next()<br />
parsed_example = tf.parse_example(example, tf.feature_column.make_parse_example_spec(feature_columns))<br />
logits = tf.feature_column.linear_model(<br />
features=parsed_example,<br />
feature_columns=feature_columns,<br />
cols_to_vars=cols_to_vars<br />
)<br />
parse_example会把TFRecord解析成一个feature dict, key是feature的名称，value是feature的值。tf.feature_column.linear_model这个API会自动生成一个线性模型，并把变量(weight)的reference, 存在cols_to_vars这个字典中。如果使用dnn的模型，可以参考tf.feature_column.input_layer这个API, 用法类似，会把feature dict转换成一个行向量。</p>
<p>模型导出<br />
使用多PS时，常见的做法是使用tf.train.replica_device_setter来把不同的变量放在不同的PS节点上。有时一个变量也会很大(这在sparse模型里很常见)，也需要拆分到不同的PS上。这时候就需要使用partitioner:</p>
<p>partitioner = tf.min_max_variable_partitioner(<br />
max_partitions=variable_partitions,<br />
min_slice_size=64 &lt;&lt; 20)<br />
with tf.variable_scope(<br />
...,<br />
partitioner = partitioner<br />
):<br />
# define model<br />
由于TensorFlow不支持sparse weight(注意: 跟sparse tensor是两码事，这里指的是被训练的变量不能是sparse的), 所以在PS上存的变量还是dense的，比如你的模型里有100亿的参数，那PS上就得存100亿。但实际上对于sparse模型，大部分的weight其实是0，最终有效的weight可能只有0.1% - 1%. 所以可以做一个优化，只导出非0参数:</p>
<p>indices = tf.where(tf.not_equal(vars_concat, 0))<br />
values = tf.gather(vars_concat, indices)<br />
shape = tf.shape(vars_concat)<br />
self.variables[column_name] = {<br />
'indices': indices,<br />
'values': values,<br />
'shape': shape,<br />
}<br />
这样就把变量的非0下标，非0值和dense shape都放进了self.variables里，之后就只导出self.variables即可。</p>
<p>此外, 在多PS的时候，做checkpoint的时候一定要记得enable sharding:</p>
<p>saver = tf.train.Saver(sharded=True, allow_empty=True)<br />
...<br />
这样在master进行checkpoint的时候，每个PS会并发写checkpoint文件。否则，所有参数会被传到master上进行集中写checkpoint, 消耗内存巨大。</p>
<p>导出优化<br />
前面提到的导出非0参数的方法仍有一个问题，就是会把参数都传到一台机器(master)上，计算出非0再保存。这一点可以由grafana上的监控验证:</p>
<p>可以看到，每隔一段时间进行模型导出时，master的网络IO就会达到一个峰值，是平时的好几倍。如果能把non_zero固定到PS上进行计算，则没有这个问题。代码如下:</p>
<p>for var in var_or_var_list:<br />
var_shape = var.<em>save_slice_info.var_shape<br />
var_offset = var.<em>save_slice_info.var_offset<br />
with tf.device(var.device):<br />
var</em> = tf.reshape(var, [-1])<br />
var_indices = tf.where(tf.not_equal(var</em>, 0))<br />
var_values = tf.gather(var_, var_indices)</p>
<pre><code>indices.append(var_indices + var_offset[0])
values.append(var_values)
</code></pre>
<p>indices = tf.concat(indices, axis=0)<br />
values = tf.concat(values, axis=0)<br />
进行了如上修改之后，master的网络消耗变成下图:</p>
<p>参考Tensorboard, 可以看到gather操作确实被放到了PS上进行运算:</p>
<p>模型上线<br />
我们目前在线上prediction时没有使用tf-serving, 而是将变量导出用Java服务进行prediction. 主要原因还是TF的PS不支持sparse存储，模型在单台机器存不下，没法使用serving. 在自己实现LR的prediction时，要注意验证线上线下的一致性，有一些常见的坑，比如没加bias, default_value没处理，等等。</p>
<p>使用TensorBoard来理解模型<br />
在训练时用TensorBoard来打出Graph，对理解训练的细节很有帮助。例如，我们试图想象这么一个问题：在分布式训练中，参数的更新是如何进行的？以下有两种方案:</p>
<p>worker计算gradients, ps合并gradients并进行更新<br />
worker计算gradients, 计算应该update成什么值，然后发给ps进行更新<br />
到底哪个是正确的？我们看一下TensorBoard就知道了: TensorBoard 可以看到，FTRL的Operation是在PS上运行的。所以是第一种。</p>
<p>Metrics<br />
在TensorFlow中有一系列metrics, 例如accuracy, AUC等等，用于评估训练的指标。需要注意的是TensorFlow中的这些metrics都是在求历史平均值，而不是当前batch。所以如果训练一开始就进行评估，使用TensorFlow的AUC和Sklearn的AUC，会发现TensorFlow的AUC会低一些，因为TensorFlow的AUC其实是考虑了历史的所有样本。正因如此Estimator的evaluate才会有一个step的参数，例如运行100个step，就会得到这100个batch上的metric.</p>
<p>性能相关<br />
profiling<br />
要进行性能优化，首先必须先做profiling。可以使用tf.train.ProfilerHook来进行.</p>
<p>hooks = [tf.train.ProfilerHook(save_secs=60, output_dir=profile_dir, show_memory=True, show_dataflow=True)]<br />
之后会写出timeline文件，可以使用chrome输入chrome://tracing来打开。</p>
<p>我们来看一个例子:  上图中IteratorGetNext占用了非常长的时间。这其实是在从HDFS读取数据。我们加入prefetch后，profile变成下面这个样子:  可以看到，加入prefetch以后HDFS读取延时和训练时间在相当的数量级，延时有所好转。</p>
<p>另一个例子:  可以看到, RecvTensor占用了大部分时间，说明瓶颈在PS上。经调查发现是partition太多(64)个而PS只有8个，造成传输数据效率下降。调整partition数量后变成如下:</p>
<p>使用Grafana进行性能监控<br />
可以使用Grafana和heapster来进行集群节点以及Pod的监控，查看CPU,内存,网络等性能指标。</p>
<h2><a id="apiversion-extensionsv1beta1-kind-deployment-metadata-name-monitoring-grafana-namespace-kube-system-spec-replicas-1-template-metadata-labels-task-monitoring-k8s-app-grafana-spec-containers-name-grafana-image-k8s-gcr-ioheapster-grafana-amd64-v4-4-3-ports-containerport-3000-protocol-tcp-volumemounts-mountpath-etcsslcerts-name-ca-certificates-readonly-true-mountpath-var-name-grafana-storage-env-name-influxdb-host-value-monitoring-influxdb-name-gf-server-http-port-value-3000-the-following-env-variables-are-required-to-make-grafana-accessible-via-the-kubernetes-api-server-proxy-on-production-clusters-we-recommend-removing-these-env-variables-setup-auth-for-grafana-and-expose-the-grafana-service-using-a-loadbalancer-or-a-public-ip-name-gf-auth-basic-enabled-value-false-name-gf-auth-anonymous-enabled-value-true-name-gf-auth-anonymous-org-role-value-admin-name-gf-server-root-url-if-you-re-only-using-the-api-server-proxy-set-this-value-instead-value-apiv1namespaceskube-systemservicesmonitoring-grafanaproxy-value-volumes-name-ca-certificates-hostpath-path-etcsslcerts-name-grafana-storage-emptydir" class="anchor" aria-hidden="true" href="#apiversion-extensionsv1beta1-kind-deployment-metadata-name-monitoring-grafana-namespace-kube-system-spec-replicas-1-template-metadata-labels-task-monitoring-k8s-app-grafana-spec-containers-name-grafana-image-k8s-gcr-ioheapster-grafana-amd64-v4-4-3-ports-containerport-3000-protocol-tcp-volumemounts-mountpath-etcsslcerts-name-ca-certificates-readonly-true-mountpath-var-name-grafana-storage-env-name-influxdb-host-value-monitoring-influxdb-name-gf-server-http-port-value-3000-the-following-env-variables-are-required-to-make-grafana-accessible-via-the-kubernetes-api-server-proxy-on-production-clusters-we-recommend-removing-these-env-variables-setup-auth-for-grafana-and-expose-the-grafana-service-using-a-loadbalancer-or-a-public-ip-name-gf-auth-basic-enabled-value-false-name-gf-auth-anonymous-enabled-value-true-name-gf-auth-anonymous-org-role-value-admin-name-gf-server-root-url-if-you-re-only-using-the-api-server-proxy-set-this-value-instead-value-apiv1namespaceskube-systemservicesmonitoring-grafanaproxy-value-volumes-name-ca-certificates-hostpath-path-etcsslcerts-name-grafana-storage-emptydir"><span class="octicon octicon-link"></span></a>apiVersion: extensions/v1beta1<br />
kind: Deployment<br />
metadata:<br />
name: monitoring-grafana<br />
namespace: kube-system<br />
spec:<br />
replicas: 1<br />
template:<br />
metadata:<br />
labels:<br />
task: monitoring<br />
k8s-app: grafana<br />
spec:<br />
containers:<br />
- name: grafana<br />
image: k8s.gcr.io/heapster-grafana-amd64:v4.4.3<br />
ports:<br />
- containerPort: 3000<br />
protocol: TCP<br />
volumeMounts:<br />
- mountPath: /etc/ssl/certs<br />
name: ca-certificates<br />
readOnly: true<br />
- mountPath: /var<br />
name: grafana-storage<br />
env:<br />
- name: INFLUXDB_HOST<br />
value: monitoring-influxdb<br />
- name: GF_SERVER_HTTP_PORT<br />
value: &quot;3000&quot;<br />
# The following env variables are required to make Grafana accessible via<br />
# the kubernetes api-server proxy. On production clusters, we recommend<br />
# removing these env variables, setup auth for grafana, and expose the grafana<br />
# service using a LoadBalancer or a public IP.<br />
- name: GF_AUTH_BASIC_ENABLED<br />
value: &quot;false&quot;<br />
- name: GF_AUTH_ANONYMOUS_ENABLED<br />
value: &quot;true&quot;<br />
- name: GF_AUTH_ANONYMOUS_ORG_ROLE<br />
value: Admin<br />
- name: GF_SERVER_ROOT_URL<br />
# If you're only using the API Server proxy, set this value instead:<br />
# value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy<br />
value: /<br />
volumes:<br />
- name: ca-certificates<br />
hostPath:<br />
path: /etc/ssl/certs<br />
- name: grafana-storage<br />
emptyDir: {}</h2>
<p>apiVersion: v1<br />
kind: Service<br />
metadata:<br />
labels:<br />
# For use as a Cluster add-on (<a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons</a>)<br />
# If you are NOT using this as an addon, you should comment out this line.<br />
kubernetes.io/cluster-service: 'true'<br />
kubernetes.io/name: monitoring-grafana<br />
name: monitoring-grafana<br />
namespace: kube-system<br />
spec:</p>
<h1><a id="in-a-production-setup-we-recommend-accessing-grafana-through-an-external-loadbalancer" class="anchor" aria-hidden="true" href="#in-a-production-setup-we-recommend-accessing-grafana-through-an-external-loadbalancer"><span class="octicon octicon-link"></span></a>In a production setup, we recommend accessing Grafana through an external Loadbalancer</h1>
<h1><a id="or-through-a-public-ip" class="anchor" aria-hidden="true" href="#or-through-a-public-ip"><span class="octicon octicon-link"></span></a>or through a public IP.</h1>
<h1><a id="type-loadbalancer" class="anchor" aria-hidden="true" href="#type-loadbalancer"><span class="octicon octicon-link"></span></a>type: LoadBalancer</h1>
<h1><a id="you-could-also-use-nodeport-to-expose-the-service-at-a-randomly-generated-port" class="anchor" aria-hidden="true" href="#you-could-also-use-nodeport-to-expose-the-service-at-a-randomly-generated-port"><span class="octicon octicon-link"></span></a>You could also use NodePort to expose the service at a randomly-generated port</h1>
<h1><a id="type-nodeport" class="anchor" aria-hidden="true" href="#type-nodeport"><span class="octicon octicon-link"></span></a>type: NodePort</h1>
<p>ports:</p>
<ul>
<li>port: 80<br />
targetPort: 3000<br />
selector:<br />
k8s-app: grafana<br />
部署到k8s集群之后，就可以分节点/Pod查看性能指标了。</li>
</ul>
<p>ps/worker数量的设置<br />
PS和worker的数量需要如何设置？这个需要大量的实验，没有绝对的规律可循。一般来说sparse模型由于每条样本的特征少，所以PS不容易成为瓶颈，worker:PS可以设置的大一些。在我们的应用中，设置到1:8会达到比较好的性能。当PS太多时variable的分片也多，网络额外开销会大。当PS太少时worker拉参数会发生瓶颈。具体的实验可以参见下节。</p>
<p>加速比实验<br />
实验方法：2.5亿样本，70亿特征(使用hash bucket后)，提交任务稳定后(20min)记录数据。</p>
<p>worker	PS	CPU/worker (cores)	CPU/PS (cores)	total CPU usage (cores)	memory/worker	memory/PS	total memory	k example/s	Network IO<br />
8	8	1.5-2	0.7-0.8	19-23	2-2.5G	15G	150G	52	0.36 GB/s<br />
16	8	1.3-2.6	1.0-1.4	40	2-2.5G	15G	170G	72	0.64 GB/s<br />
32	8	1.8-2.3	1.5-2.0	70	2.2-3.0G	15G	216G	130	1.2 GB/s<br />
64	8	1.5-2	4.0-6.4	150	2.2-3G	15G	288G	250	2.3 GB /s<br />
64	16	1.7	2.9	169	2.2G	7.5G	300G	260	2.5 GB/s<br />
128	16	1.5-2	3.9-5	278	2.2G	7.5G	469G	440	4.2 GB/s<br />
128	32	1.5	2.5-3	342	2.2G	4-4.5G	489G	420	4.1 GB/s<br />
160	20	1.2-1.5	3.0-4.0	360	2-2.5G	6-7G	572G	500	4.9 GB/s<br />
160	32	1.2-1.5	1.0-1.4	388	2-2.5G	4-4.5G	598G	490	4.8 GB/s<br />
结论:</p>
<p>worker消耗的内存基本是固定的；PS消耗的内存只跟特征规模有关;<br />
worker消耗的CPU跟训练速度和数据读取速度有关; PS消耗的CPU和worker:PS的比例有关;<br />
TensorFlow的线性scalability是不错的, 加速比大约可以到0.7-0.8;<br />
最后加到160worker时性能提升达到瓶颈，这里的瓶颈在于HDFS的带宽(已经达到4.9GB/s)<br />
在sparse模型中PS比较难以成为瓶颈，因为每条样本的特征不多，PS的带宽相对于HDFS的带宽可忽略不计<br />
Future Work<br />
文件分配优化<br />
如果worker平均需要1个小时的时间，那么整个训练需要多少时间？如果把训练中的速度画一个图出来，会是类似于下面这样:</p>
<p>在训练过程中我发现，快worker的训练速度大概是慢worker的1.2-1.5倍。而整个训练的时间取决于最慢的那个worker。所以这里可以有一个优化，就是动态分配文件，让每个worker尽可能同时结束。借助之前提到的zookeeper, 做到这一点不会很难。</p>
<p>online learning<br />
在online learning时，其实要解决的问题也是任务分配的问题。仍由master借助zookeeper进行任务分配，让每个worker去读指定的消息队列分区。</p>
<p>PS优化<br />
可以考虑给TensorFlow增加Sparse Variable的功能，让PS上的参数能够动态增长，这样就能解决checkpoint, 模型导出慢等问题。当然，这样肯定也会减慢训练速度，因为变量的保存不再是数组，而会是hashmap.</p>
<p>样本压缩<br />
从前面的实验看到，后面训练速度上不去主要是因为网络带宽瓶颈了，读样本没有这么快。我们可以看看TFRecord的实现：</p>
<p>message Example {<br />
Features features = 1;<br />
};</p>
<p>message Feature {<br />
// Each feature can be exactly one kind.<br />
oneof kind {<br />
BytesList bytes_list = 1;<br />
FloatList float_list = 2;<br />
Int64List int64_list = 3;<br />
}<br />
};</p>
<p>message Features {<br />
// Map from feature name to feature.<br />
map&lt;string, Feature&gt; feature = 1;<br />
};<br />
可以看到，TFRecord里面其实是保存了key和value的，在sparse模型里，往往value较小，而key很大，这样就造成很大的浪费。可以考虑将key的长度进行压缩，减少单条样本的大小，提高样本读取速度从而加快训练速度。</p>


</div>

<br /><br />
<hr />

<div class="row clearfix">
  <div class="large-6 columns">
	<div class="text-left" style="padding:15px 0px;">
		
	        <a href="15808187045354.html"  title="Previous Post: pytorch 环境安装及配置">&laquo; pytorch 环境安装及配置</a>
	    
	</div>
  </div>
  <div class="large-6 columns">
	<div class="text-right" style="padding:15px 0px;">
		
	        <a href="15768070172080.html" 
	        title="Next Post: 基础知识背景">基础知识背景 &raquo;</a>
	    
	</div>
  </div>
</div>

<div class="row">
<div style="padding:0px 0.93em;" class="share-comments">

</div>
</div>
<script type="text/javascript">
	$(function(){
		var currentURL = '15768353446694.html';
		$('#side-nav a').each(function(){
			if($(this).attr('href') == currentURL){
				$(this).parent().addClass('active');
			}
		});
	});
</script>  
</div></div>


<div class="page-bottom">
  <div class="row">
  <hr />
  <div class="small-9 columns">
  <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
  <div class="small-3 columns">
  <p class="copyright text-right"><a href="#header">TOP</a></p>
  </div>
   
  </div>
</div>

        </section>
      </div>
    </div>
    



  













<script src="asset/prism.js"></script>


<style type="text/css">
figure{margin: 0.4em 0;padding: 0;}
  figcaption{text-align:center;}

/* PrismJS 1.14.0
 http://prismjs.com/download.html#themes=prism&languages=markup+css+clike+javascript */
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
    color: black;
    background: none;
    text-shadow: 0 1px white;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
    
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
    
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
    text-shadow: none;
    background:#b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
    text-shadow: none;
    background: #b3d4fc;
}

@media print {
    code[class*="language-"],
    pre[class*="language-"] {
        text-shadow: none;
    }
}

/* Code blocks */
pre[class*="language-"] {
    padding: 1em;
    margin: .5em 0;
    overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
    background: #F7F7F7;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
    padding: .1em;
    border-radius: .3em;
    white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
    color: slategray;
}

.token.punctuation {
    color: #999;
}

.namespace {
    opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
    color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
    color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
    color: #9a6e3a;
    background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
    color: #07a;
}

.token.function,
.token.class-name {
    color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
    color: #e90;
}

.token.important,
.token.bold {
    font-weight: bold;
}
.token.italic {
    font-style: italic;
}

.token.entity {
    cursor: help;
}


pre[class*="language-"].line-numbers {
    position: relative;
    padding-left: 3.8em;
    counter-reset: linenumber;
}

pre[class*="language-"].line-numbers > code {
    position: relative;
    white-space: inherit;
}

.line-numbers .line-numbers-rows {
    position: absolute;
    pointer-events: none;
    top: 0;
    font-size: 100%;
    left: -3.8em;
    width: 3em; /* works for line-numbers below 1000 lines */
    letter-spacing: -1px;
    border-right: 1px solid #999;

    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;

}

    .line-numbers-rows > span {
        pointer-events: none;
        display: block;
        counter-increment: linenumber;
    }

        .line-numbers-rows > span:before {
            content: counter(linenumber);
            color: #999;
            display: block;
            padding-right: 0.8em;
            text-align: right;
        }


</style>
    
    <script src="asset/js/foundation.min.js"></script>
    <script src="asset/js/foundation/foundation.offcanvas.js"></script>
    <script>
      $(document).foundation();

     
    </script>


  </body>
</html>
