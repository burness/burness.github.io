<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  图计算 - 小石头的码疯窝
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="小石头的码疯窝" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
 
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site: ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="https://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; 小石头的码疯窝</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html">推荐系统</a></li>
        
            <li><a href="TensorFlow.html">TensorFlow</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0.html">机器学习平台</a></li>
        
            <li><a href="%E5%9B%BE%E8%AE%A1%E7%AE%97.html">图计算</a></li>
        
            <li><a href="pytorch.html">pytorch</a></li>
        
            <li><a href="%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F.html">分布式系统</a></li>
        
            <li><a href="mlops.html">mlops</a></li>
        
            <li><a href="paddle.html">paddle</a></li>
        
            <li><a href="%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8.html">参数服务器</a></li>
        
            <li><a href="GAN.html">GAN</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="16078481811620.html">
                
                  <h1>直播场景PGL落地</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>作为国民级的音乐APP， 网易云音乐很早之前就将定位从传统的音乐工具软件转换为音乐内容社区，连接好泛音乐产品与用户，做最懂用户的音乐APP。</p>
<p>而直播作为参与度较多的场景， 云音乐内部花费大量的功夫将合适的主播推荐给用户。</p>
<h2><a id="%E8%AF%84%E8%AE%BA%E9%A1%B5%E7%9B%B4%E6%92%AD%E6%8E%A8%E8%8D%90%E9%9A%BE%E7%82%B9" class="anchor" aria-hidden="true" href="#%E8%AF%84%E8%AE%BA%E9%A1%B5%E7%9B%B4%E6%92%AD%E6%8E%A8%E8%8D%90%E9%9A%BE%E7%82%B9"><span class="octicon octicon-link"></span></a>评论页直播推荐难点</h2>
<p>###业务难点<br />
<img src="media/16078481811620/16078518146489.jpg" alt="" /></p>
<p>如图，是评论页场景直播推荐，和经典的推荐架构，主要包括召回、粗排、精排，本次实践主要落地在召回，而在此场景，我们约到的核心问题在于阅读评论页的用户较大部分未产生过行为，因而采用传统的协同过滤类算法，由于数据极度稀疏，并不能产生较好的效果，为典型的用户冷启动问题。</p>
<p>###基建难点</p>
<p>目前云音乐的所有计算资源已完成容器化部署， 在成本考虑下， 不可能给各个业务团队，比如384G内存、8卡的计算资源来进行海量数据的图神经网络模型训练， 如何在有限的、分布式的资源调控策略下完成大规模的图神经网络的训练也是我们必须要考虑的</p>
<h2><a id="%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E5%8D%8F%E5%8A%A9%E8%90%BD%E5%9C%B0" class="anchor" aria-hidden="true" href="#%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E5%8D%8F%E5%8A%A9%E8%90%BD%E5%9C%B0"><span class="octicon octicon-link"></span></a>图神经网络如何协助落地</h2>
<h3><a id="%E8%A1%8C%E4%B8%BA%E5%9F%9F%E7%9F%A5%E8%AF%86%E8%BF%81%E7%A7%BB" class="anchor" aria-hidden="true" href="#%E8%A1%8C%E4%B8%BA%E5%9F%9F%E7%9F%A5%E8%AF%86%E8%BF%81%E7%A7%BB"><span class="octicon octicon-link"></span></a>行为域知识迁移</h3>
<p>所幸，并不是没有解决方法，新用户在直播场景下为新，但是在音乐、歌单、mlog，通常会有较多的行为，是否能考虑这部分的知识，来映射到直播域，提供足够的“行为”来推荐合适的主播呢？<br />
这里我们考虑使用Graph的结构， 引入多种不同类型的实体，如歌曲、DJ、Query、RadioID等等，将用户与主播、用户与歌曲、Query与主播等等行为，统一建模在一张图中，通过经典的Graph Model如DeepWalk， Metapath2vec、GraphSAGE,学习足够强大的embedding表示，来建模各实体id，然后通过ANN召回，通过用户对歌曲、Query等等的行为迁移至主播域，召回合适的主播。</p>
<h3><a id="%E9%80%9A%E7%94%A8%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88" class="anchor" aria-hidden="true" href="#%E9%80%9A%E7%94%A8%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="octicon octicon-link"></span></a>通用的分布式图神经网络解决方案</h3>
<p>由于该场景整体数据规模较大，在经过某些规则裁剪之后， 整体数据规模约为百万级节点，亿级别边，而在很多开源方案中，这样的量级就已经开始成为瓶颈，或者使用极其昂贵的计算资源来解决， 这个对于我们现有的资源情况来说是不可能， 而在我们的规划中，这样的数据量级仅仅还只是开始，所以，我们在开始之初， 就考虑到数据规模的问题，相对于使用什么的模型，我们更关注其在工业界场景数据下是否能训练得出， 通过调研现有的开源方案，我们选择PGL作为我们基础的框架， PGL基于PaddlePaddle Fleet来支持Graph Learning中较为独特的Distributed Graph Storage、Distributed Sampling， 可以较为方便的通过上层python接口，来将graph中的存储比如Side Feature等存储在不同server上，也支持通用的分布式采样接口，将不同子图的采样分布式处理，在分布式的“瘦计算节点”上加速计算， 这对于我们是十分具有吸引力。</p>
<p><img src="media/16078481811620/16078503357215.jpg" alt="" /></p>
<h3><a id="%E8%BF%9B%E5%B1%95%E4%B8%8E%E6%9C%AA%E6%9D%A5%E8%A7%84%E5%88%92" class="anchor" aria-hidden="true" href="#%E8%BF%9B%E5%B1%95%E4%B8%8E%E6%9C%AA%E6%9D%A5%E8%A7%84%E5%88%92"><span class="octicon octicon-link"></span></a>进展与未来规划</h3>
<p>目前采用图计算的实验，在有效观看上提升将近6%， 在该场景下尤其是通过引入其他域数据上来解决新用户冷启动问题上，有明显提升，未来，在这块上，我们规划从以下三个方向上探索</p>
<ol>
<li>模型上：目前我们已经完成包括DeepWalk（带权重）、MetaPath2Vec（带权重）、GraphSAGE直播场景的落地，后续打算引入更复杂的模型如attention 机制是否能协助解决不同实体类型对最终实体建模的贡献能力；</li>
<li>数据规模：通过不停挖掘更多行为数据、关联更多行为实体， 数据规模越来越大，从单机到分布式，PGL目前支持较为完好，后续我们打算减少数据阈值，引入更多合适的领域数据来探索是否能够进一步提升；</li>
<li>平台化能力输出：目前PGL的分布式方案尽管已在云音乐内部的机器学习平台上运行，但并未整合到提供平台化能力输出， 用户暂时无法自助运行分布式的PGL模型训练，也需要更改部分源码来适配分布式训练，后期我们打算通过制作组件以及对PGL进行封装，支持PGL训练代码无需修改代码来分布式训练；</li>
</ol>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/12/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%9B%BE%E8%AE%A1%E7%AE%97.html'>图计算</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="16077533959046.html">
                
                  <h1>Graph Neural Networks</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p><img src="media/16072205939601/16077444867227.jpg" alt="" /></p>
<p>经典的机器学习框架主要支持包括Images和Text/Speech 这两种数据，主要设计用来解决sequence、grids问题， 而Graph 结构相对较为复杂：<br />
<img src="media/16072205939601/16077446484476.jpg" alt="" /></p>
<ul>
<li>无法产生特定顺序的节点序列，因而很难转换为普通序列问题；</li>
<li>图结构频繁变化，且通常需要建模多模态特征；</li>
</ul>
<h2><a id="basics-of-deep-learning-for-graphs" class="anchor" aria-hidden="true" href="#basics-of-deep-learning-for-graphs"><span class="octicon octicon-link"></span></a>Basics of deep learning for graphs</h2>
<p>假定存在图G，其中V表示节点集合，A是邻接矩阵，\(X \in R^{m*|V|}\)是节点的特征矩阵（a matrix of node features）。这里的节点特征，根据不同的网络有不同的定义——如在社交网络里面，节点特征就包括用户资料、用户年龄等；在生物网络里面，节点的特征就包括基因表达、基因序列等；如果节点没有特征，就可以用one-hot编码表示或者常数向量 1: [ 1 , 1 , … , 1 ] [1, 1, …, 1][1,1,…,1]来表示节点的特征。</p>
<h3><a id="local-network-neighborhoods" class="anchor" aria-hidden="true" href="#local-network-neighborhoods"><span class="octicon octicon-link"></span></a>Local Network neighborhoods</h3>
<pre><code>节点的邻居定义计算图， 能够有效地建模信息的传播；
</code></pre>
<p><img src="media/16072205939601/16077467780136.jpg" alt="" /></p>
<p>如上图，根据节点邻居得到计算图， 然后根据计算图生成节点的向量表示，如下图，A的节点表示由其邻居节点{B,C,D}，而这些节点又由其邻居节点决定，形成如下图的计算图，其中方形框即为其聚合邻居节点的策略，可采用average操作：<br />
<img src="media/16072205939601/16077469131380.jpg" alt="" /></p>
<p>由此，可将图中任意节点，根据其邻居节点，生成对应的计算图。</p>
<h3><a id="stacking-multiple-layers" class="anchor" aria-hidden="true" href="#stacking-multiple-layers"><span class="octicon octicon-link"></span></a>Stacking multiple layers</h3>
<p>上一小节示意图中，使用节点的最高至二阶邻居点，即邻居的邻居来生成某节点的计算图，也可以使用任意深度的模型来表示，即图神经网络中层的概念：</p>
<ul>
<li>任意节点在每一层均有对应的向量表示；</li>
<li>layer-0的向量表示即模型的输入特征\(x_{u}\);</li>
<li>layer-k的向量表示，即节点经过k层邻居之后的向量表示；<br />
因而，Graph Neural Network能够通过叠加layer建模更高阶信息。</li>
</ul>
<p>Graph Neural Network整体架构到这里其实基本上就讲完了，而不同的上述各种图中的方形框内部的计算逻辑。</p>
<p><img src="media/16072205939601/16077481762719.jpg" alt="" /></p>
<p>一个有效的方法即GCN的思路是， 在入口对接收到的message平均化， 多阶的信息由邻居的邻居的信息传播来进行建模， 和常见的深度学习模型没啥差别：</p>
<p><img src="media/16072205939601/16077482635491.jpg" alt="" /></p>
<h3><a id="%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83" class="anchor" aria-hidden="true" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="octicon octicon-link"></span></a>模型训练</h3>
<p>模型训练基本方式和常规的深度学习方式基本一致， 构建神经网络结构，通过构造loss函数，使用梯度下降方法，对模型参数进行更新，以最小化loss函数值：<br />
<img src="media/16072205939601/16077483702242.jpg" alt="" /></p>
<h4><a id="%E9%9D%9E%E7%9B%91%E7%9D%A3%E6%96%B9%E5%BC%8F" class="anchor" aria-hidden="true" href="#%E9%9D%9E%E7%9B%91%E7%9D%A3%E6%96%B9%E5%BC%8F"><span class="octicon octicon-link"></span></a>非监督方式</h4>
<p>基于图神经网络的非监督方式相对于传统的深度学习方式更加流行，其通常利用graph结构信息（其实也可以拼接slide info，slide未提），保证在graph中有相似的节点，有相似的特征表示。常见的模型包括：random walk系列（node2vec， DeepWalk， Struc2vec）， Graph Factorization， Node Proximity in the graph</p>
<h4><a id="%E6%9C%89%E7%9B%91%E7%9D%A3%E6%96%B9%E5%BC%8F" class="anchor" aria-hidden="true" href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E6%96%B9%E5%BC%8F"><span class="octicon octicon-link"></span></a>有监督方式</h4>
<p>有监督方式的loss函数，根据不同任务类型来定义，如在常见的节点分类任务，如互联网中判断某节点是否为薅羊毛党，其为二元分类问题，通常采用交叉熵来表征其loss函数：<br />
<img src="media/16072205939601/16077487568893.jpg" alt="" /></p>
<h4><a id="%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%E5%9B%9E%E9%A1%BE" class="anchor" aria-hidden="true" href="#%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%E5%9B%9E%E9%A1%BE"><span class="octicon octicon-link"></span></a>模型设计回顾</h4>
<p><strong>定义邻居聚合函数与loss函数:</strong><br />
<img src="media/16072205939601/16077488653617.jpg" alt="" /><br />
<strong>按邻居节点生成批计算图</strong><br />
<img src="media/16072205939601/16077489286319.jpg" alt="" /><br />
<strong>生成图节点的向量表示</strong><br />
<img src="media/16072205939601/16077489762539.jpg" alt="" /></p>
<p><img src="media/16072205939601/16077490853979.jpg" alt="" /><br />
对于计算图中，同一layer中的节点采用共享的参数表示，其参数彼此共享，且针对新图或者图中的新出现节点，均同样适用。</p>
<h2><a id="graph-convolutional-networks-and-graphsage" class="anchor" aria-hidden="true" href="#graph-convolutional-networks-and-graphsage"><span class="octicon octicon-link"></span></a>Graph Convolutional Networks and GraphSAGE</h2>
<h3><a id="%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D" class="anchor" aria-hidden="true" href="#%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D"><span class="octicon octicon-link"></span></a>基本介绍</h3>
<p>如前文所述，在邻居节点聚合操作时，我们采用average操作，那么是否有更多通用的方式呢？GraphSAGE的思路是采用一个通用的aggregation函数来表示方形框的计算，并将其与本身的向量表示拼接起来，aggregation函数要保证可微分。</p>
<p><img src="media/16072205939601/16077501660049.jpg" alt="" /></p>
<p>常见的Aggregation如下：</p>
<ul>
<li>
<p>Mean： <img src="media/16072205939601/16077502322567.jpg" alt="" /></p>
</li>
<li>
<p>Pool: <img src="media/16072205939601/16077502450481.jpg" alt="" /></p>
</li>
<li>
<p>LSTM: <img src="media/16072205939601/16077502557065.jpg" alt="" /></p>
</li>
</ul>
<h3><a id="%E5%9B%9E%E9%A1%BE" class="anchor" aria-hidden="true" href="#%E5%9B%9E%E9%A1%BE"><span class="octicon octicon-link"></span></a>回顾</h3>
<p>如下图， 图神经网络通过建模邻居节点的方式来解决图节点向量化的难题，通过多层高阶graph layer来聚合不同阶邻居的信息，并构建Graph计算图，来训练得到节点向量化表示，其中GCN为最基础方式，其聚合函数采取平均策略，而GraphSAGE设计通用的邻居聚合函数，如mean、pool、LSTM等等。<br />
<img src="media/16072205939601/16077504561113.jpg" alt="" /><br />
关于GNN的一些论文：</p>
<pre><code>Tutorials and overviews:
§ Relational inductive biases and graph networks (Battaglia et al., 2018)
§ Representation learning on graphs: Methods and applications (Hamilton et al., 2017)
Attention-based neighborhood aggregation:
§ Graph attention networks (Hoshen, 2017; Velickovic et al., 2018; Liu et al., 2018)
Embedding entire graphs:
§ Graph neural nets with edge embeddings (Battaglia et al., 2016; Gilmer et. al., 2017)
§ Embedding entire graphs (Duvenaud et al., 2015; Dai et al., 2016; Li et al., 2018) and graph pooling
(Ying et al., 2018, Zhang et al., 2018)
§ Graph generation and relational inference (You et al., 2018; Kipf et al., 2018)
§ How powerful are graph neural networks(Xu et al., 2017)
Embedding nodes:
§ Varying neighborhood: Jumping knowledge networks (Xu et al., 2018), GeniePath (Liu et al., 2018) § Position-aware GNN (You et al. 2019)
Spectral approaches to graph neural networks:
§ Spectral graph CNN &amp; ChebNet (Bruna et al., 2015; Defferrard et al., 2016) § Geometric deep learning (Bronstein et al., 2017; Monti et al., 2017)
Other GNN techniques:
§ Pre-training Graph Neural Networks (Hu et al., 2019)
§ GNNExplainer: Generating Explanations for Graph Neural Networks (Ying et al., 2019)
</code></pre>
<h2><a id="graph-attention-networks" class="anchor" aria-hidden="true" href="#graph-attention-networks"><span class="octicon octicon-link"></span></a>Graph Attention Networks</h2>
<p>GAT顾名思义，需要考虑邻居节点中，对最终结果的不同的影响，因而引入attention机制，attention在经典的深度学习网络中十分常见，如transformer的self-attention，</p>
<h3><a id="attention-mechanism" class="anchor" aria-hidden="true" href="#attention-mechanism"><span class="octicon octicon-link"></span></a>Attention Mechanism</h3>
<p>如下图， \(e_{uv}\)可以通过两个节点之间的信息流得到, \(e_{vu}\)表示节点u的信息对节点v的重要性, 使用softmax进行归一化，以便在不同的节点间存在可比性，然后集成在模型聚合函数中来建模不同节点对当前节点信息的重要性。<br />
<img src="media/16072205939601/16077513339656.jpg" alt="" /><br />
其他另外过程和原先GCN、graphSAGE没什么差别，除了attention信息也会参与训练。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/12/12</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%9B%BE%E8%AE%A1%E7%AE%97.html'>图计算</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="16077533959085.html">
                
                  <h1>graph representation learning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2><a id="%E5%86%99%E5%9C%A8net-embedding%E4%B9%8B%E5%89%8D" class="anchor" aria-hidden="true" href="#%E5%86%99%E5%9C%A8net-embedding%E4%B9%8B%E5%89%8D"><span class="octicon octicon-link"></span></a>写在net embedding之前</h2>
<p>一个标准的机器学习流程如下：<br />
<img src="media/16059739825319/16060364526425.jpg" alt="" style="width:1239px;" /></p>
<p>其中如何设计一套合理方式来高效地进行特征表示，是十分重要的，比如在cv与nlp任务中，我们会分别设计cnn模块与RNN模块来建模图像中像素点表征的信息、word表征的信息。</p>
<p><img src="media/16059739825319/16060365758008.jpg" alt="" style="width:891px;" /><br />
那么在graph中，如何进行特征表示呢 ？ Graph的特征表示极为复杂，主要表现在以下三个方面：</p>
<ul>
<li>极其复杂的拓扑结构，很难简单地像图像中的感受野来提取有效信息；</li>
<li>无特定的节点顺序；</li>
<li>通常graph会是动态变化的， 且使用多模态特征；<br />
<img src="media/16059739825319/16060366418527.jpg" alt="" style="width:992px;" /></li>
</ul>
<p>今天我们就来了解下如何对graph进行特征学习；</p>
<h2><a id="embedding-nodes" class="anchor" aria-hidden="true" href="#embedding-nodes"><span class="octicon octicon-link"></span></a>Embedding Nodes</h2>
<p>假设有图G，V表示节点集合，A为对应的邻接矩阵，embedding node是对图当中的节点进行有效地编码，保证相似的节点在编码也有类似的相似性，如下图：<br />
<img src="media/16059739825319/16060371472308.jpg" alt="" style="width:1162px;" /></p>
<p>节点的向量学习分为以下三个方面：</p>
<ol>
<li>定义编码器， 即ENC(v)，将图中的节点通过周边结构关系的学习，映射到低维的向量表示，如\(Z_{v}\)，在常见的任务中，通常就是一个embedding矩阵，通过lookup拿到对应节点的向量，然后反向进行embedding向量的更新，这个即成为学习；</li>
<li>定义相似度函数sim(u,v)，即如何定义两个节点相似度的大小；</li>
<li>通过数据的学习不断来优化编码器参数，保证\(sim(u,v) = Z_{v}^{T}Z_{u}\)；</li>
</ol>
<h3><a id="random-walk" class="anchor" aria-hidden="true" href="#random-walk"><span class="octicon octicon-link"></span></a>Random Walk</h3>
<p>何谓&quot;随机游走&quot;？给定一个graph和一个起点，随机选择其一个邻居，并移动到这个邻居，然后继续随机选择这个点的邻居，产生随机的点序列，而点和点之间出现的概率即为\(Z_{v}^{T}Z_{u}\)<br />
<img src="media/16059739825319/16060378237593.jpg" alt="" style="width:1213px;" /></p>
<p>Random Walk有两个特点：</p>
<ul>
<li>极具表达性： 节点相似计算的定义、随机，且包含了局部（邻居）以及高阶邻居的信息；</li>
<li>效率级高：训练时不需要穷举所有的节点对，只需要考虑随机游走中同时的对；</li>
</ul>
<h3><a id="%E9%9D%9E%E7%9B%91%E7%9D%A3%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0" class="anchor" aria-hidden="true" href="#%E9%9D%9E%E7%9B%91%E7%9D%A3%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0"><span class="octicon octicon-link"></span></a>非监督特征学习</h3>
<p>给定图G=（V， E）， 学习一种映射\(z: u-&gt;R^{d}\)，目标函数：<br />
<img src="media/16059739825319/16060382044872.jpg" alt="" style="width:643px;" /><br />
其中，\(N_{R}(u)\)表明以策略R得到的节点u的邻居：</p>
<p><strong>优化</strong></p>
<ol>
<li>使用某种策略R，从图上的每个节点开始进行固定长度的较短的random walks；</li>
<li>对于每个节点u，得到其的邻居\(N_{R}(u)\);</li>
<li>根据上图中似然函数，对embedding进行优化；</li>
</ol>
<p>这里，将目标函数，修改为以下，其中\(p(v|z_{u})\)可通过softmax得到：<br />
<img src="media/16059739825319/16060384652033.jpg" alt="" style="width:1139px;" /><br />
但是一旦使用softmax，其复杂度变得极大，你必须计算graph当中所有的节点，其复杂度达到了\(O(|V|^2)\)，和大家十分熟悉的word2vec，我们采用Negative Sampling来近似计算：</p>
<p><strong>Negative Sampling</strong></p>
<p><img src="media/16059739825319/16060386578960.jpg" alt="" style="width:1252px;" /></p>
<p>Negative Sampling仅仅随机选择k个负样本进行归一化操作，其中\(P_V\)是所有节点的随机分布，其中k属于超参，通常有以下经验：</p>
<ul>
<li>k越大，预估有更强的鲁棒性；</li>
<li>k越大，负样本偏差会越大；</li>
<li>k通常使用5-20；</li>
</ul>
<h3><a id="node2vec" class="anchor" aria-hidden="true" href="#node2vec"><span class="octicon octicon-link"></span></a>Node2vec</h3>
<p>Node2vec解决的和random类似的额问题， Node2vec扩展节点u的邻居\(N_{R}(u)\)的定义，来丰富embedding的建模信息；</p>
<p><strong>Biased Walk</strong><br />
DeepWalk选取随机游走序列中下一个节点的方式是均匀随机分布的，而node2vec通过引入两个参数p和q，其中p为返回到上一个节点的概率， q表示生成策略选择DFS或者BFS的概率， 将宽度优先搜索和深度优先搜索引入随机游走序列的生成过程。宽度优先搜索(BFS)注重临近的节点，并刻画了相对局部的一种网络表示，宽度优先中的节点一般会出现很多次，从而降低刻画中心节点的邻居节点的方差；深度优先搜索(BFS)反应了更高层面上的节点间的同质性。（即BFS能够探究图中的结构性质，而DFS则能够探究出内容上的相似性（相邻节点之间的相似性）。其中结构相似性不一定要相连接，甚至可能相距很远。<br />
<img src="media/16059739825319/16060392560279.jpg" alt="" style="width:1086px;" /></p>
<p>得到Walks之后， Node2vec算法和DeepWalk基本类似，整体流程如下：</p>
<ul>
<li>计算随机游走的概率；</li>
<li>模型从每个节点u开始的步长为l的r次随机游走；</li>
<li>利用优化方法优化目标函数；</li>
</ul>
<h3><a id="%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%8A%82%E7%82%B9%E7%9A%84embedding%EF%BC%9F" class="anchor" aria-hidden="true" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%8A%82%E7%82%B9%E7%9A%84embedding%EF%BC%9F"><span class="octicon octicon-link"></span></a>如何使用节点的embedding？</h3>
<p>训练好的节点向量可用于很多场景：</p>
<ul>
<li>比如专栏前面文章提到的聚类、社区检测；</li>
<li>节点分类，比如根据节点embedding预测节点是否属于薅羊毛人群，新闻实体是否为fake news；</li>
<li>关系预测，比如预测用于是否与对应实体可能产生连接关系，即是否可能产生行为关系；</li>
</ul>
<h2><a id="%E6%80%BB%E7%BB%93" class="anchor" aria-hidden="true" href="#%E6%80%BB%E7%BB%93"><span class="octicon octicon-link"></span></a>总结</h2>
<p>本章课程，主要介绍了在graph中做node embedding的基本概念，以及常见的方法如Random Walk、Node2vec，并简要介绍了embedding的用途，视频教程中还有基于KG的Translating Embedding的应用，因为讲的过于简单，仅仅介绍TranE的三元组关系，感觉并不能理清楚其实际落地细节，所以本文不做描述，这部分相关工作建议直接阅读TranE的相关文档，比如药物中蛋白质分子的应用的等等，其实本文将是接下来各种复杂GNN的入门课程，大概所有的深度学习模型其实都在学习一种有效地特征表示，如本章内容而言，仅是入门，还有很多有意思的工作，比如是否能建模graph的dynamic的信息，行为是动态更新的，dynamic在某些场景下更能表征节点的社交行为结构化信息，另外还有包括side information的引入，都是在graph下的embedding node下很重要的工作。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/12/12</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%9B%BE%E8%AE%A1%E7%AE%97.html'>图计算</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="16029890553575.html">
                
                  <h1>Spectral Clustering</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li><a href="#graph-partitioning">Graph Partitioning</a>
<ul>
<li><a href="#criterion">Criterion</a>
<ul>
<li><a href="#minimum-cut">Minimum-cut</a></li>
<li><a href="#conductance">Conductance</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#spectral-graph-partitioning">Spectral Graph Partitioning</a>
<ul>
<li><a href="#matrix-representations">Matrix Representations</a></li>
<li><a href="#find-optimal-cut">Find Optimal Cut</a></li>
</ul>
</li>
<li><a href="#spectral-clustering-algorithm">Spectral Clustering Algorithm</a>
<ul>
<li><a href="#%E5%9F%BA%E7%A1%80%E6%96%B9%E6%B3%95">基础方法</a></li>
<li><a href="#k-way-spectral-clustering">k-Way Spectral Clustering</a></li>
</ul>
</li>
<li><a href="#motif-based-spectral-clustering">Motif-Based SPectral Clustering</a>
<ul>
<li><a href="#motif-conductance">Motif Conductance</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
</ul>

<h2><a id="graph-partitioning" class="anchor" aria-hidden="true" href="#graph-partitioning"><span class="octicon octicon-link"></span></a>Graph Partitioning</h2>
<p>何谓graph partitioning, 如下图，给定无向图\(G(V,E)\)， 将这些节点分为两个组：</p>
<p><img src="media/16029890553575/16029968236086.jpg" alt="" /></p>
<p>逻辑很简单，但是难点在于：</p>
<ol>
<li>如何定义一个尺度，来保证图的切分是合理的：
<ol>
<li>组内成员连接尽可能多；</li>
<li>组与组之间连接尽可能少；</li>
</ol>
</li>
<li>如何高效地识别这些分区；</li>
</ol>
<h3><a id="criterion" class="anchor" aria-hidden="true" href="#criterion"><span class="octicon octicon-link"></span></a>Criterion</h3>
<p><strong>Cut(A,B):</strong> 如下图，图当中，两个点分别在两个分组的边的数量；</p>
<p><img src="media/16029890553575/16030072315012.jpg" alt="" /></p>
<h4><a id="minimum-cut" class="anchor" aria-hidden="true" href="#minimum-cut"><span class="octicon octicon-link"></span></a>Minimum-cut</h4>
<p>最小化图分组间的连接（如果有权重，则考虑权重）：</p>
\[arg min_{A, B}\ Cut(A,B)
\]
<p>这样会存问题：</p>
<ul>
<li>仅仅考虑图当中分组的外部连接；</li>
<li>未考虑图中分组的内部连接；<br />
因此，在下面图中，会出现，假如是minimum cut不是optimal cut<br />
<img src="media/16029890553575/16030073776880.jpg" alt="" /></li>
</ul>
<h4><a id="conductance" class="anchor" aria-hidden="true" href="#conductance"><span class="octicon octicon-link"></span></a>Conductance</h4>
<p>与Minimum-cut逻辑不一样， Conductance不仅仅考虑分组间的连接， 也考虑了分割组内的“体积块”， 保证分割后得到的块更均衡，Conductance指标如下：</p>
\[	\phi(A, B)=\frac{cut(A,B)}{min(vol(A), vol(B))}
\]
<p>其中\(vol(A)\)指分组块A内节点所有的权重度之和；<br />
但是，得到最好的Conductance是一个np难题。</p>
<h2><a id="spectral-graph-partitioning" class="anchor" aria-hidden="true" href="#spectral-graph-partitioning"><span class="octicon octicon-link"></span></a>Spectral Graph Partitioning</h2>
<p>假定A为无连接图G的链接矩阵表示，如(i,j)中存在边，则\(A_{ij}=1\)，否则为0；<br />
假定x是维度为n的向量\((x_1, ..., x_n)\)，我们认为他是图当中每个节点的一种标签；<br />
那么\(A*x\)的意义是， 如下图， \(y_i\)表示i的邻居节点与对应标签和：<br />
<img src="media/16029890553575/16030084889666.jpg" alt="" /></p>
<p>令\(Ax=\lambda x\)，可以得到特征值：\(\lambda_i\), 和对应的特征向量\(x_i\)。对于图G， spectral(谱)定义为对应特征值\({\lambda_1, \lambda_2, ..., \lambda_n}\)，其\(\lambda_1 \leq \lambda_2 \leq ... \leq \lambda_n\) 对应的特征向量组\({x_i, x_2, ..., x_n}\)；</p>
<p>** d-Regular Graph 举例 **</p>
<p>假定图当中每个节点的度均为\(d\)，且G是连通的，即称为\(d-Regular Graph\)。<br />
假定\(x = (1,1,...,1)\)，那么\(Ax = (d, d, ..., d) = \lambda x\)， 故会有对应的特征对：</p>
\[x=(1,1,...,1), \lambda =d
\]
<p>且d是A最大的特征值（证明课程未讲）</p>
<p><strong>d-Regular Graph on 2 Components</strong></p>
<p>假定G有两个部分， 每个部分均为d-Regular Graph，<br />
那么必然存在：</p>
<p>\(x^{'}=(1,...,1,0,...,0)^T\)， \(A x^{'}=(d,...,d,0,...,0)^T\)<br />
\(x^{''}=(0,...,0,1,...,1)^T\)， \(A x^{'}=(0,...,0,d,...,d)^T\)</p>
<p>所以必然存在两个特征值\(\lambda_{n} = \lambda_{n-1}\)， 推广起来，如果图G中两个部分互相连通，如下图， 则最大的特征值很近似：<br />
<img src="media/16029890553575/16030100066742.jpg" alt="" /></p>
<p>推广， 这里有点没有太理解：<br />
<img src="media/16029890553575/16030113720635.jpg" alt="" /></p>
<h3><a id="matrix-representations" class="anchor" aria-hidden="true" href="#matrix-representations"><span class="octicon octicon-link"></span></a>Matrix Representations</h3>
<p><strong>邻接矩阵A</strong></p>
<ul>
<li>对称矩阵；</li>
<li>n个实数特征值；</li>
<li>特征向量均为实数向量且正交:<br />
<img src="media/16029890553575/16030114397493.jpg" alt="" /></li>
</ul>
<p><strong>度矩阵</strong></p>
<ul>
<li>对角矩阵；</li>
<li>\(D=[d_{ii}]\), \(d_{ii}\) 表示节点i的度；<br />
<img src="media/16029890553575/16030115840965.jpg" alt="" /></li>
</ul>
<p><strong>Laplacian matrix</strong><br />
<img src="media/16029890553575/16030117088486.jpg" alt="" /></p>
<p>Laplacian matrix 有以下特点：</p>
<ul>
<li>令x=(1,...,1)则\(L*x=0\)， 故\(\lambda=\lambda_{1}=0\)；</li>
<li>L的特征值均为非负实数；</li>
<li>L的特征向量均为实数向量，且正交；</li>
<li>对于所有x，\(x^{T}Lx=\sum_{ij} L_{ij}x_{i}x_{j} \geq 0\)；</li>
<li>L能够表示为\(L = N^{T} N\)</li>
</ul>
<h3><a id="find-optimal-cut" class="anchor" aria-hidden="true" href="#find-optimal-cut"><span class="octicon octicon-link"></span></a>Find Optimal Cut</h3>
<p>分组表示(A,B)为一个向量，其中<br />
<img src="media/16029890553575/16030132613466.jpg" alt="" /><br />
问题转换为寻找最小化各部分间连接：<br />
<img src="media/16029890553575/16030135083034.jpg" alt="" /></p>
<p>相关证明间slide，这里老师没有做过多解读；</p>
<h2><a id="spectral-clustering-algorithm" class="anchor" aria-hidden="true" href="#spectral-clustering-algorithm"><span class="octicon octicon-link"></span></a>Spectral Clustering Algorithm</h2>
<h3><a id="%E5%9F%BA%E7%A1%80%E6%96%B9%E6%B3%95" class="anchor" aria-hidden="true" href="#%E5%9F%BA%E7%A1%80%E6%96%B9%E6%B3%95"><span class="octicon octicon-link"></span></a>基础方法</h3>
<p>如下图：主要包括三个步骤：</p>
<ul>
<li>预处理：构造图的表示， 包括Laplacian Matrix；</li>
<li>矩阵分解：
<ul>
<li>计算Laplacian Matrix的所有的特征值与特征向量；</li>
<li>将节点使用特征向量表示（对应\(\lambda_2\)的特征向量\(x_2\)）；<br />
<img src="media/16029890553575/16030136483959.jpg" alt="" /></li>
</ul>
</li>
<li>聚类， 将节点的特征表示，排序， 按大于0与小于来进行拆分：<br />
<img src="media/16029890553575/16030139525112.jpg" alt="" /></li>
</ul>
<p>以下是多个实例， 看起来使用\(\lambda_{2}\)对应的特征向量\(x_2\)来切分是比较合适的：</p>
<p><img src="media/16029890553575/16030141846384.jpg" alt="" /><br />
<img src="media/16029890553575/16030141957860.jpg" alt="" /></p>
<h3><a id="k-way-spectral-clustering" class="anchor" aria-hidden="true" href="#k-way-spectral-clustering"><span class="octicon octicon-link"></span></a>k-Way Spectral Clustering</h3>
<p>如何将图切分为k个聚类呢？</p>
<ul>
<li>递归利用二分算法，将图进行划分。但是递归方法效率比较低，且比较不稳定；</li>
<li>使用降维方法，将节点表示为低维度的向量表示，然后利用k-mean类似的方法对节点进行聚类；</li>
</ul>
<p>那么如何选择合适的k呢，如下图，计算连续的特征值之间的差值，选择差异最大的即为应该选择的k？<br />
<img src="media/16029890553575/16030144584423.jpg" alt="" /></p>
<h2><a id="motif-based-spectral-clustering" class="anchor" aria-hidden="true" href="#motif-based-spectral-clustering"><span class="octicon octicon-link"></span></a>Motif-Based SPectral Clustering</h2>
<p>是否能够通过专有的pattern 来进行聚类呢？上一篇文章有提到motif， 如下图：<br />
<img src="media/16029890553575/16030146086801.jpg" alt="" /><br />
给定motif，是否能够得到相应地聚类结果：</p>
<p><img src="media/16029890553575/16030146698741.jpg" alt="" /></p>
<p>答案当然是可以的， 而且也是复用前面的逻辑</p>
<h3><a id="motif-conductance" class="anchor" aria-hidden="true" href="#motif-conductance"><span class="octicon octicon-link"></span></a>Motif Conductance</h3>
<p>和上文中， 按边来切分逻辑不通， conductance指标，应该表征为motif的相关指标，如下：<br />
<img src="media/16029890553575/16030147436743.jpg" alt="" /><br />
这里给出一个计算的例子， 如下图， 该出模式分子为切分经过的该模式数量， 分母为该模式覆盖的所有节点数量：<br />
<img src="media/16029890553575/16030149022677.jpg" alt="" style="width:412px;" /></p>
<p>所以motif的谱聚类就变成了给定图G与Motif结构来找到\(\phi_{M}(S)最小的\)， 很不幸， 找到最小化motif conductance也是一个np问题；<br />
同样地，也专门提出了解决motif 谱聚类的方法：</p>
<ol>
<li>给定图G和motif M；</li>
<li>按M和给定的G，生成新的权重图\(W_{(M)}\);</li>
<li>在新的图上应用spectral clustering方法；</li>
<li>输出对应的类簇；</li>
</ol>
<p>大致过程如下图所示：</p>
<p><img src="media/16029890553575/16030170812738.jpg" alt="" /><br />
具体过程如下：</p>
<ol>
<li>
<p>给定图G与motif M， 计算权重图\(W^(M)\)：<br />
<img src="media/16029890553575/16030175212616.jpg" alt="" /></p>
</li>
<li>
<p>应用谱聚类， 计算其Laplacian Matrix的特征值与特征向量，得到第二小的特征向量，：<br />
<img src="media/16029890553575/16030175521379.jpg" alt="" /></p>
</li>
<li>
<p>按升序对第二小特征值的对应的特征向量进行排序（对应的节点ID需要保存以计算motif conductance）， 以\(S_r = {x_1, ...,x_r}\)计算motif conductance值，选择最小地的值即为划分点， 如下图，1,2,3,4,5为一个类：<br />
<img src="media/16029890553575/16030177764796.jpg" alt="" /></p>
</li>
</ol>
<h2><a id="summary" class="anchor" aria-hidden="true" href="#summary"><span class="octicon octicon-link"></span></a>Summary</h2>
<p>本章我们学习了谱聚类相关的工作， 首先，讲了关于表征切分图的指标cut(A,B)以及conductance，如何切分图以及为什么切分图是一个np难题，然后提出了利用谱聚类的方法来解决该问题，从而学习到了degree matrix, Laplacian matrix等概念； 而后提出是否有按motif来进行图聚类的方法， 并基于谱聚类的方法来解决来转换原图为带权重的图来解决；</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/10/18</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%9B%BE%E8%AE%A1%E7%AE%97.html'>图计算</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="16019601035128.html">
                
                  <h1>Community structure in networks</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2><a id="granovetter-s-theory" class="anchor" aria-hidden="true" href="#granovetter-s-theory"><span class="octicon octicon-link"></span></a>Granovetter's theory</h2>
<p>马克·格兰诺维特（Mark Granovetter，1943年10月20日－），美国社会学家，斯坦福大学教授。格兰诺维特是论文被引用最多的学者之一，根据 Web of Science 的数据，社会学论文被引数排名第一和第三的文章皆出自格兰诺维特之手。格兰诺维特因为对社会网络和经济社会学的研究而成名。其最著名成就是1974年提出的<strong>弱连接理论</strong>：与自己频繁接触的亲朋好友之间是一种“强连接”，通过这种连接获取到的往往是<strong>同质性</strong>的信息；但社会上更为广泛的是一种并不深入的人际关系，这种弱关系能够使个体获得通过强关系无法获取到的信息，从而在工作和事业上、在信息的扩散上起到决定作用。</p>
<p><img src="media/16019601035128/16019671729435.jpg" alt="" /><br />
格兰诺维特的研究认为如果两个人之间有共同的朋友，那他们成为朋友的可能性较大。</p>
<p>格兰诺维特的研究也在真实的数据上得到了验证</p>
<h3><a id="edge-overlap" class="anchor" aria-hidden="true" href="#edge-overlap"><span class="octicon octicon-link"></span></a>Edge Overlap</h3>
<p>简单解释下，Edge Overlap表示两个节点的邻居节点的重合程度（本身节点不在计算范围内），下图中右边部分右上图， \(N(i)=4, N(j)=4\)， 去除本身i, j 所以\(N(i) \cup N(j) = 6\), \(N(i) \cap N(j) = 2\)， 所以\(O_{ij}=1/3\)<br />
<img src="media/16019601035128/16019673807440.jpg" alt="" /></p>
<p>Edge Overlap被用来当做节点间链接的强弱的一种度量，通过在实际数据集（欧洲通话网络数据集得到验证）， 在实际数据中，具有高边重叠的边，确实是有着强连接关系的， 这里的强连接关系即用节点间通话次数来表示关系强弱；</p>
<h3><a id="%E7%A4%BE%E5%8C%BA%E5%86%85%E5%BC%BA%E8%BF%9E%E6%8E%A5%EF%BC%8C%E7%A4%BE%E5%8C%BA%E9%97%B4%E5%BC%B1%E8%BF%9E%E6%8E%A5" class="anchor" aria-hidden="true" href="#%E7%A4%BE%E5%8C%BA%E5%86%85%E5%BC%BA%E8%BF%9E%E6%8E%A5%EF%BC%8C%E7%A4%BE%E5%8C%BA%E9%97%B4%E5%BC%B1%E8%BF%9E%E6%8E%A5"><span class="octicon octicon-link"></span></a>社区内强连接，社区间弱连接</h3>
<p>上面的研究表明，在图中确实会存在紧密连接的社群概念，社群内的链接基本是强连接， 而社群间的连接是弱连接，强链接偏向将信息流锁紧在社群内部，而边缘连接由于涉及到多个社群之间，在信息传播上更有优势<br />
<img src="media/16019601035128/16019678718742.jpg" alt="" /></p>
<h2><a id="%E7%BD%91%E7%BB%9C%E7%A4%BE%E7%BE%A4%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5" class="anchor" aria-hidden="true" href="#%E7%BD%91%E7%BB%9C%E7%A4%BE%E7%BE%A4%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="octicon octicon-link"></span></a>网络社群的基本概念</h2>
<p>网络中的社群的基础定义：紧密连接的节点集合，这些节点间有较多的内部连接，而相对较少的外部连接；</p>
<p>Zachary空手道俱乐部一个在图这块入门级的数据集，用来展示图网络中基本的问题如节点分类、社区发现等等；</p>
<p>如下图， 仅靠图的结构化关系，可以比较合理地将俱乐部进行切分，即规划至各自的社群：<br />
<img src="media/16019601035128/16019684859829.jpg" alt="" /><br />
另一个例子是NCAA FootBall Network：</p>
<p><img src="media/16019601035128/16019687374850.jpg" alt="" /></p>
<h3><a id="%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A4%BE%E7%BE%A4%EF%BC%9F" class="anchor" aria-hidden="true" href="#%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A4%BE%E7%BE%A4%EF%BC%9F"><span class="octicon octicon-link"></span></a>如何寻找网络中的社群？</h3>
<p><strong>Modularity Q</strong>用来衡量网络中社群划分的指标， 其基础含义如下：</p>
\[Q \Rightarrow	\sum_{s \in S}[(\#\ edges\ within\ group\ s) - (expected\ \#\ edges\ with\ in\ group\ s) ]
\]
<p>其中\((expected\ \#\ edges\ with\ in\ group\ s)\)需要构建null model（之前的学习笔记有提到过Configuration Model， 忘记了的小伙伴可以翻一下）， 保证相同的degree distribution且连接概率为均匀随机， 假定两个节点i、j，其度分别为\(k_i, k_j\)，那么节点间边的期望为\(p(i,j)= \frac{k_{i}k_{j}}{2m}\)，所以这个图里面所有边的期望为：</p>
\[E_{edge}=0.5 \sum_{i \in N} \sum_{j \in N} \frac{k_{i}k_{j}}{2m}= 0.5 * \frac{1}{2m}\sum_{i \in N}k_{i}(\sum_{j \in N} k_{j})=\frac{1}{4m}* 2m*2m = m
\]
<p>所以Modularity Q从：</p>
\[Q \Rightarrow	\sum_{s \in S}[(\#\ edges\ within\ group\ s) - (expected\ \#\ edges\ with\ in\ group\ s) ]
\]
<p>表示为：</p>
\[Q(G, S) = \frac{1}{2m}\sum_{s \in S}\sum_{i \in s}\sum_{j \in s}(A_{ij}-\frac{k_{i}k_{j}}{2m})
= \frac{1}{2m}\sum_{ij}[A_{ij}-\frac{k_{i}k_{j}}{2m}]\delta(c_i, c_j)
\]
<p>其\(A_{ij}\ =\ 1\ if\ i-&gt;j\ 0\ otherwise, \delta(c_i, c_j)=1\ if\ c_i = c_j\ 0\ otherwise\)</p>
<p>Q值域为[-1, 1], 正值表示社群内的边多于期望， 当Q为0.3-0.7之间表明有显著的社群结构；</p>
<h3><a id="louvain-algorithm" class="anchor" aria-hidden="true" href="#louvain-algorithm"><span class="octicon octicon-link"></span></a>Louvain Algorithm</h3>
<p>Louvain Algorithm是一个最大化Modularity Q的贪心算法，主要由两步构成：</p>
<h4><a id="%E8%BF%87%E7%A8%8B1%EF%BC%9Apartitioning" class="anchor" aria-hidden="true" href="#%E8%BF%87%E7%A8%8B1%EF%BC%9Apartitioning"><span class="octicon octicon-link"></span></a>过程1：Partitioning</h4>
<ol>
<li>初始化的时候给所有节点分配一个单独的社区；</li>
<li>针对每一个节点i，进行以下步骤：
<ol>
<li>计算改几点被划分到其邻居节点j所在的社群，并且计算该社群的modularity delta；</li>
<li>将节点i移入至最好modularity delta的社群；</li>
</ol>
</li>
<li>迭代，直到modularity delta不再增加；</li>
</ol>
<p>其中modularity delta \(\Delta Q\)， 应该包括\(\Delta (i-&gt;C)\)(将节点i移入community C增加的Q值)与\(\Delta(D-&gt;i)\)（将节点i溢出Community D）， 因为\(\\Delta(D-&gt;i)\)， 在步骤1中都一样， 所以\(\Delta Q\)公式主要依赖于\(\Delta (i-&gt;C)\)， 如下：</p>
\[\Delta Q(i -&gt;C)[\frac{\sum_{in} { k_{i, in}}}{2m} - (\frac{\sum_{tot}+k_i}{2m})^2] - [\frac{\sum_{in}}{2m} - (\frac{\sum_{tot}}{2m})^2 - (\frac{k_i}{2m})^2]
\]
<p>其中：<br />
\(\sum_{in}\) 表示社群内节点之间的边权重和（社群内边）；<br />
\(\sum_{tot}\) 表示社群中节点所有的边的边权重和（社群内边与社群外边均考虑）；<br />
\(k_{i, in}\) 表示社群中节点i与其他节点的边权重和；<br />
\(k_i\) 表示节点i连接的所有边的权重和；</p>
<h4><a id="%E8%BF%87%E7%A8%8B2%EF%BC%9A-restructuring" class="anchor" aria-hidden="true" href="#%E8%BF%87%E7%A8%8B2%EF%BC%9A-restructuring"><span class="octicon octicon-link"></span></a>过程2： Restructuring</h4>
<p>经过第一步之后， 会出现很多超级节点，这些超级节点间所在的社群间如果有任意的边连接，那么我们要连接超级节点， 超级节点间的边的权重等于他们相应地社群间所有边权重的和；</p>
<p>具体的伪代码如图，逻辑还是相对比较简单的：</p>
<p><img src="media/16019601035128/16019735628276.jpg" alt="" /></p>
<h3><a id="%E5%9B%9E%E9%A1%BE%E4%B8%8E%E7%9C%9F%E5%AE%9E%E6%95%B0%E6%8D%AE%E5%B1%95%E7%A4%BA" class="anchor" aria-hidden="true" href="#%E5%9B%9E%E9%A1%BE%E4%B8%8E%E7%9C%9F%E5%AE%9E%E6%95%B0%E6%8D%AE%E5%B1%95%E7%A4%BA"><span class="octicon octicon-link"></span></a>回顾与真实数据展示</h3>
<p>每一个pass 包含：partitioning和restructuring 两个部分， 如下，迭代直到收敛：<br />
<img src="media/16019601035128/16019737571940.jpg" alt="" /></p>
<p>真实数据集上的一个例子：<br />
<img src="media/16019601035128/16019738811453.jpg" alt="" /></p>
<h2><a id="%E6%A3%80%E6%B5%8B%E6%9C%89%E9%87%8D%E5%8F%A0%E7%9A%84%E7%A4%BE%E7%BE%A4%E7%AE%97%E6%B3%95%EF%BC%9Abigclam" class="anchor" aria-hidden="true" href="#%E6%A3%80%E6%B5%8B%E6%9C%89%E9%87%8D%E5%8F%A0%E7%9A%84%E7%A4%BE%E7%BE%A4%E7%AE%97%E6%B3%95%EF%BC%9Abigclam"><span class="octicon octicon-link"></span></a>检测有重叠的社群算法：BigCLAM</h2>
<h3><a id="%E4%BD%95%E8%B0%93%E6%9C%89%E9%87%8D%E5%8F%A0%E7%9A%84%E7%A4%BE%E7%BE%A4%EF%BC%9F" class="anchor" aria-hidden="true" href="#%E4%BD%95%E8%B0%93%E6%9C%89%E9%87%8D%E5%8F%A0%E7%9A%84%E7%A4%BE%E7%BE%A4%EF%BC%9F"><span class="octicon octicon-link"></span></a>何谓有重叠的社群？</h3>
<p><strong>无重叠的社群与有重叠的社群</strong><br />
<img src="media/16019601035128/16019740253790.jpg" alt="" /></p>
<p>很多实际数据集中，存在有重叠的社群：<br />
<strong>facebook的Ego-Network</strong><br />
<img src="media/16019601035128/16019741473152.jpg" alt="" /></p>
<p><strong>PPI</strong><br />
<img src="media/16019601035128/16019743028655.jpg" alt="" /></p>
<h3><a id="bigclam" class="anchor" aria-hidden="true" href="#bigclam"><span class="octicon octicon-link"></span></a>BigCLAM</h3>
<p>BigCLAM主要步骤如下：</p>
<ol>
<li>基于节点所属的社群，构造一个图生成模型，构造方法为AGM(Community Affiliation Graph Model)；</li>
<li>根据我们实际的图G，假设它是用AGM生成的。寻找一个最优的AGM，能产生我们实际的图G， 通过这个AGM，确定每个节点所属的社区</li>
</ol>
<h4><a id="community-affiliation-graph-model" class="anchor" aria-hidden="true" href="#community-affiliation-graph-model"><span class="octicon octicon-link"></span></a>Community Affiliation Graph Model</h4>
<p>如下图， 左边部分主要包括节点V， 社群C， 从属关系，其中每一个社群的概率为\(p_c\)：</p>
<ol>
<li>社群c内的节点，彼此连接形成边的概率是\(p_c\);</li>
<li>对于从属与多个社群的节点，如果其在一个社群内没有连接， 其边在另外社群的连接也是可能存在的；</li>
</ol>
<p><img src="media/16019601035128/16019745985681.jpg" alt="" /></p>
<p>AGM可以生成多种不同的社群结构，如Non-overlapping, Overlapping, Nested：<br />
<img src="media/16019601035128/16019749871418.jpg" alt="" /></p>
<h4><a id="detecting-communities-with-agm" class="anchor" aria-hidden="true" href="#detecting-communities-with-agm"><span class="octicon octicon-link"></span></a>Detecting Communities with AGM</h4>
<p>使用AGM来做社群发现就转换成一个给定graph G， 如何反推AGM的各个参数F：Affiliation Graph M， 社群个数C，以及社群内连接概率\(p_c\)</p>
<p>给定G和F， 计算器似然函数:</p>
\[P(G|F)=\prod_{(u,v) \in G}P(u, v)\prod_{(u,v) \not \in G}(1-p(u,v))
\]
<p>上式中前一个连乘表示图当中所有的边的似然函数，后一个表示不在该graph边集合的似然。<br />
考虑到每个节点属于各个社区的权重是不同的， 向量\(F_u\)表示节点u属于各个社群的权重，加上log转换， 所以log似然函数为：</p>
\[l(F) = \sum_{(u,v) \in E} log(1-exp(-F_{u}F_{u}^{T})) - \sum_{(u,v) \not \in E}F_{u}F_{v}^T
\]
<p>这里和lr的梯度下降很类似，其实就是一样，只需要用梯度下降公司待进去自然就可以算出收敛是的F的参数；</p>
<p>有了F之后， 我们的节点从属社群关系就极其明显了。</p>
<h2><a id="%E6%80%BB%E7%BB%93" class="anchor" aria-hidden="true" href="#%E6%80%BB%E7%BB%93"><span class="octicon octicon-link"></span></a>总结</h2>
<p>今天，课程上主要讲解的是graph中社群的概念，通过Granovetter's theory和真实数据的对比，验证了图当中的强弱连接，从而引出社群概念，然后介绍了一种简单的社群发现算法Louvain Algorithm，通过最大化Modularity Q来保证节点与社群的从属， 最后提供可重叠的社群发现，提出BigCLAM算法，通过最大化log似然函数优化AGM生成对应真实Graph，来得到AGM， 从而得到包括社群从属关系在内的各个参数，从而识别节点从属；</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/10/06</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%9B%BE%E8%AE%A1%E7%AE%97.html'>图计算</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="16017096445579.html">
                
                  <h1>Motifs and structural roles in networks</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li><a href="#subgraphs">Subgraphs</a>
<ul>
<li><a href="#motif-induced-subgraphs">Motif: Induced Subgraphs</a></li>
<li><a href="#motifs-recurrence">Motifs: Recurrence</a></li>
<li><a href="#motif%EF%BC%9A-significance">Motif： Significance</a></li>
<li><a href="#configuration-model">Configuration Model</a></li>
<li><a href="#alternative-for-spokes-switching">Alternative for Spokes: Switching</a></li>
<li><a href="#%E6%9C%AC%E8%8A%82%E6%80%BB%E7%BB%93">本节总结</a></li>
</ul>
</li>
<li><a href="#graphlets-node-feature-vectors">Graphlets: Node Feature Vectors</a></li>
<li><a href="#finding-motifs-and-graphlets">Finding Motifs and Graphlets</a>
<ul>
<li><a href="#exact-subgraph-enumeration">Exact Subgraph Enumeration###</a></li>
<li><a href="#graph-isomorphism">Graph Isomorphism</a></li>
</ul>
</li>
<li><a href="#structural-roles-in-networks">Structural roles in networks</a>
<ul>
<li><a href="#discovering-structural-roles-in-networks">Discovering Structural Roles in Networks</a>
<ul>
<li><a href="#roix-automatic-discovery-of-nodes-structural-roles-in-network">RoIX: AutoMatic Discovery of nodes' structural roles in network</a></li>
</ul>
</li>
</ul>
</li>
</ul>

<h2><a id="subgraphs" class="anchor" aria-hidden="true" href="#subgraphs"><span class="octicon octicon-link"></span></a>Subgraphs</h2>
<p>何谓motif？ 图中反复出现的相互连接的模式，有以下三个特点：</p>
<ul>
<li>Pattern：小的能导出的子图；</li>
<li>Recurring：频繁出现；</li>
<li>Significant：模式的出现明显高于预期，如类似的random genderated networks中的模式；</li>
</ul>
<h3><a id="motif-induced-subgraphs" class="anchor" aria-hidden="true" href="#motif-induced-subgraphs"><span class="octicon octicon-link"></span></a>Motif: Induced Subgraphs</h3>
<p><strong>Induced Subgraph</strong>如下图所示，图中红色框虽然也是3个节点构成的子图，但是该子图与待匹配的子图不匹配（连接不一致），而蓝色的三角框中的子图与待匹配的子图匹配， 匹配的意思是指必须是出现在待匹配子图里所有节点的边，如果不是待匹配节点之间的边，则不匹配；<br />
<img src="media/16017096445579/16017113719209.jpg" alt="" /></p>
<h3><a id="motifs-recurrence" class="anchor" aria-hidden="true" href="#motifs-recurrence"><span class="octicon octicon-link"></span></a>Motifs: Recurrence</h3>
<p>如下图，右侧图中出现了4个待匹配的motif，motif之间可以相互重叠；<br />
<img src="media/16017096445579/16017117035269.jpg" alt="" /></p>
<h3><a id="motif%EF%BC%9A-significance" class="anchor" aria-hidden="true" href="#motif%EF%BC%9A-significance"><span class="octicon octicon-link"></span></a>Motif： Significance</h3>
<p>如下图， 该motif在真实的网络中出现的频率要对类似的随机网络出现的频率要高的多， 我们成为其显著性明显；<br />
<img src="media/16017096445579/16017120299603.jpg" alt="" /></p>
<p>显著性通常是和随机性网络做对比，通常使用\(Z_i\)来描述motif i的显著性， 其中\(N_{i}^{real}\)表示真实网络中motif i的数量， \(N_{i}^{rand}\)表示在随机网络中motif i的数量：</p>
\[Z_i = \frac{(N_{i}^{real}-N_{avg\ i}^{rand})}{std(N_{i}^{rand})}
\]
<p>上面的计算会随着网络规模的不同而有数值的变化，而大的网络会倾向于有更大的Z-score， 归一化处理之后，使用<strong>Net significance profile</strong>来表示，motif i的SP计算公式如下：</p>
\[SP_{i}= \frac{Z_{i}}{\sqrt{\sum_{j}{Z_{j}^{2}}}}
\]
<h3><a id="configuration-model" class="anchor" aria-hidden="true" href="#configuration-model"><span class="octicon octicon-link"></span></a>Configuration Model</h3>
<p>配置一个和真实网络相同的度序列的随机图可以分为三步：</p>
<ol>
<li>按节点的度序列生成Node spokes；</li>
<li>随机从nodes spokes中挑选两个连接起来；</li>
<li>根据源节点和目标节点，将步骤2中聚合起来，即形成和真实网络相同度序列的随机网络；</li>
</ol>
<p><img src="media/16017096445579/16017133097101.jpg" alt="" /></p>
<h3><a id="alternative-for-spokes-switching" class="anchor" aria-hidden="true" href="#alternative-for-spokes-switching"><span class="octicon octicon-link"></span></a>Alternative for Spokes: Switching</h3>
<p>另一个产生于源图类似的图的方法就是随机做边交换，具体步骤如下：</p>
<ol>
<li>从源图中随机找出边如，A-&gt;B, C-&gt;D，随机交换边的终点产生边A-&gt;D, C-&gt;B， 如果交换导致自己指向自己，则不交换；</li>
<li>重复1中，Q次， 当Q足够大时，即可生成随机图；</li>
</ol>
<h3><a id="%E6%9C%AC%E8%8A%82%E6%80%BB%E7%BB%93" class="anchor" aria-hidden="true" href="#%E6%9C%AC%E8%8A%82%E6%80%BB%E7%BB%93"><span class="octicon octicon-link"></span></a>本节总结</h3>
<p>经过上面的定义与解释之后，我们就可以定义如何检测一个motif：</p>
<ol>
<li>在真实图中，统计induced subgraph的个数；</li>
<li>统计生成的随机网络中的induced subgraph的个数， 这里随机生成的网络，可以生成多个做对比；</li>
<li>计算Z-score， 那些高的Z-score就是我们需要的motif；</li>
</ol>
<p>motif也有相应的变种， 如不同的频率概念、不同的显著性计算标准、null model的不同约束等等，但基本上都是万变不离其宗；</p>
<h2><a id="graphlets-node-feature-vectors" class="anchor" aria-hidden="true" href="#graphlets-node-feature-vectors"><span class="octicon octicon-link"></span></a>Graphlets: Node Feature Vectors</h2>
<p>Graphlet是基础的由节点构成的基础子图单位，由两个节点开始，下图是2-5个节点的graphlet示例图：<br />
<img src="media/16017096445579/16017149303054.jpg" alt="" /><br />
那么，如何由graphlet来改造节点的特征呢？<br />
之前我们提到的度，是指每个节点能够接触到的边数，这里我们扩展Graphlet degress vector，用来表示节点v能够接触到的graphlet的数量， 如下图，graphlet有a, b, c, d, 注意这里d和c画在一张图上, 这样我就可以用2, 1, 0, 2来表示节点V：<br />
<img src="media/16017096445579/16017150379200.jpg" alt="" /></p>
<p>Graphlet degree vector的意义在与它提供了对于一个节点的本地网络拓扑的度量，这样可以比较两个节点的GDV来度量它们的相似度。由于Graphlet的数量随着节点的增加可以很快变得非常大，所以一般会选择2-5个节点的Graphlet来标识一个节点的GDV。</p>
<h2><a id="finding-motifs-and-graphlets" class="anchor" aria-hidden="true" href="#finding-motifs-and-graphlets"><span class="octicon octicon-link"></span></a>Finding Motifs and Graphlets</h2>
<p>在一个图里识别出一种特定大小的motifs和graphlet，并计算它的数量是非常难的一个问题。识别是否是同构子图本身就是一个NP-hard的问题：计算量也会随着节点数的增加而呈指数增长，因此， 一般只识别节点数较小如3-8的motif或者graphlet。</p>
<h3><a id="exact-subgraph-enumeration" class="anchor" aria-hidden="true" href="#exact-subgraph-enumeration"><span class="octicon octicon-link"></span></a>Exact Subgraph Enumeration###</h3>
<p>ESU从一个节点\(v\)开始，算法分为两个集合：\(V_{subgraph}\):表示当前构造的子图，\(V_{extension}\)：表示能够扩展motif的候选节点， 将满足以下两个条件的节点\(u\)加入到\(V_{extension}\)：</p>
<ol>
<li>\(u\)的节点id要大于v的id；</li>
<li>\(u\)只能是新加入加点\(w\)的邻居，而不能是\(V_{subgraph}\)里的节点邻居；</li>
</ol>
<p>伪代码逻辑如下：<br />
<img src="media/16017096445579/16017169702979.jpg" alt="" /></p>
<p>如下图就很容易理解了， 从不同的点出发， 比如node 1， node 1的邻居只有3， 所以开始extendsion为3， node2， 其邻居只有3， 所以extension为3， node 3， 其邻居有1、2、4、5，为保证id要大于node 3， 所以extension为4、5， node 4， 邻居有3、5， 保证id大于4所以extension为5， 第二层将exetension加入到subgraph， 且此时exetension要是新加入节点，如最左边分支，新加入节点为3，exetension要讲node 3的邻居加入，且保证大于node1 ， 即exetension变为2，4，5；左起第二个， 将exetension 中node 3加入， node3的邻居节点有1、2、4、5， 其中只有4、5大于原先subgraph中的2， 所有exetension为4、5， 以此类推即可，最终所有大小为3的子图即可遍历出来；<br />
<img src="media/16017096445579/16017171725519.jpg" alt="" /></p>
<p>到目前为止， 我们就可以遍历出所有大小为的子图， 接下来我们只需要统计下这些图即可， 如下图所示， 需要判断是否为同构图，即拓扑结构完全一致：<br />
<img src="media/16017096445579/16017182837335.jpg" alt="" /></p>
<h3><a id="graph-isomorphism" class="anchor" aria-hidden="true" href="#graph-isomorphism"><span class="octicon octicon-link"></span></a>Graph Isomorphism</h3>
<p>如何判定两个图是同构？<br />
如果图\(G\)和\(H\)是同构的，那么必定存在一个双向映射\(f: V_{(G}-&gt;H_{(H)}\)保证任意两个节点u和v在图G里面是相邻的，则\(f ( u )\) 和\(f ( v )\)在图H里也是相邻的, 检查图是否重构是一个NP难题</p>
<p><img src="media/16017096445579/16017185650015.jpg" alt="" /></p>
<h2><a id="structural-roles-in-networks" class="anchor" aria-hidden="true" href="#structural-roles-in-networks"><span class="octicon octicon-link"></span></a>Structural roles in networks</h2>
<p>Role: 角色， 是对节点在网络中功能的描述， 是有相同结构特征的点，相同角色的节点并不一定直接相连，而Group/Communities(社群)， 是彼此相互密集连接的节点群；</p>
<p>视频中举了个例子，假定一个计算机系构建一个社交网络，其中：</p>
<ul>
<li>角色指： 教职、职员、学生；</li>
<li>社群指： AILab、Info Lab、 Theory Lab等；</li>
</ul>
<p><img src="media/16017096445579/16017190758770.jpg" alt="" /></p>
<p>如果节点u和节点v和所有其他节点有相同的关系，则说明节点u和节点v在<strong>结构上等同</strong>， 如下图中u和v完全相同；<br />
<img src="media/16017096445579/16017191551998.jpg" alt="" /></p>
<h3><a id="discovering-structural-roles-in-networks" class="anchor" aria-hidden="true" href="#discovering-structural-roles-in-networks"><span class="octicon octicon-link"></span></a>Discovering Structural Roles in Networks</h3>
<p>为什么要研究图当中的role ？如下图：</p>
<p><img src="media/16017096445579/16017193904639.jpg" alt="" /></p>
<h4><a id="roix-automatic-discovery-of-nodes-structural-roles-in-network" class="anchor" aria-hidden="true" href="#roix-automatic-discovery-of-nodes-structural-roles-in-network"><span class="octicon octicon-link"></span></a>RoIX: AutoMatic Discovery of nodes' structural roles in network</h4>
<p>RoIX特点如下：</p>
<ul>
<li>非监督学习方法；</li>
<li>无需先验知识；</li>
<li>支持多种角色分类；</li>
<li>按边数线性扩展；</li>
</ul>
<p><img src="media/16017096445579/16017339015292.jpg" alt="" /></p>
<p>RoIX过程如下图， 其中最重要的Recursive Feature Extraction.<br />
<img src="media/16017096445579/16017339215442.jpg" alt="" /><br />
Recursive Feature Extraction是基于图的结构详细，从某一节点出发，聚合该节点的特征，如有向图中，该特征未出度、入度、度等等，其次基于该node的邻居、包含该节点的可导出子图，这称之为Egonet，也会提取Egonet中节点的特征。以此类推，用这种方法提取到的特征，是指数级增长，后续会使用裁剪技术将部分特征裁剪掉；<img src="media/16017096445579/16017347294623.jpg" alt="" /></p>
<p>最终，每一个节点会由如下图的向量表示， 然后采用non negative matrix factorization（KL离散度距离来评估似然度） 即可完成流程图中node * role matrix与role * feature matrix的生成：<br />
<img src="media/16017096445579/16017347796677.jpg" alt="" /></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/10/03</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%9B%BE%E8%AE%A1%E7%AE%97.html'>图计算</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="16016274752497.html">
                
                  <h1>Properties of Networks and Random Graph Model</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2><a id="%E5%A6%82%E4%BD%95%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C" class="anchor" aria-hidden="true" href="#%E5%A6%82%E4%BD%95%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C"><span class="octicon octicon-link"></span></a>如何描述一个网络</h2>
<h3><a id="degree-distribution" class="anchor" aria-hidden="true" href="#degree-distribution"><span class="octicon octicon-link"></span></a>Degree Distribution</h3>
<p>P(k): 随机选择的节点， 度为k的的概率分布， 使用直方图来描述</p>
<p><img src="media/16016274752497/16016288312820.jpg" alt="" /><br />
其中\(N_k\)表示度为k的节点数， 比如上图中，度为1的节点数有6， 所有节点数为10， 所以\(P(6)=0.6\)</p>
<h3><a id="path-length" class="anchor" aria-hidden="true" href="#path-length"><span class="octicon octicon-link"></span></a>Path Length</h3>
<p>Path: path是指每个节点连接下一个节点的序列，其中，一个path能够重复多次相同的边， 如下图： ACBDCDEG<br />
<img src="media/16016274752497/16016290595027.jpg" alt="" /></p>
<p>Distance: 连接节点对最少数量的边，称为两个节点间的distance，如下图，其中\(h_{B,D}=2, h_{A,X}=\infty\)， 若图中两节点无连接，或中间连接断开，则distance为无穷，在有向图中，distance的计算应该考虑两个节点间的方向，如下图\(h_{B,C}=1, h{C,B}=2\)，不是对称的：<br />
<img src="media/16016274752497/16016293620828.jpg" alt="" /><br />
<img src="media/16016274752497/16016295608758.jpg" alt="" /></p>
<p><strong>Diameter</strong>在graph中，所有节点对当中最长distance；<br />
<strong>Average path length</strong>针对graph来说， average path length计算公式如下：</p>
\[h_{average}=\frac{\sum_{i,j!=i h_{ij}}}{2E_{max}}
\]
<p>其中\(h_{ij}\)是node i到node j的distance, \(E_{max}\)是指图最多可存在的边数：\(\frac{n(n-1)}{2}\)</p>
<h3><a id="cluster-coefficient" class="anchor" aria-hidden="true" href="#cluster-coefficient"><span class="octicon octicon-link"></span></a>Cluster coefficient</h3>
<p><strong>cluster coefficient</strong> 对于无向图，用来描述节点i与他的邻居的链接情况， 其中节点i的度为\(k_{i}\)，clustering coefficient计算公式如下：</p>
\[C_{i}=\frac{2e_{i}}{k_{i}(k_{i}-1)}
\]
<p>如下图， 图的node i的cluster coefficient计算如下：\(C_{i}=\frac{2*6}{4*(4-1)}=1, C_{i}=\frac{(2*3)}{4*(4-1)}=0.5, C_{i}=\frac{2*0}{4*(4-1)}=0\)<br />
Average clustering coefficient: \(C=\frac{1}{N}\sum_{i}^{N}C_{i}\)<br />
<img src="media/16016274752497/16016303918942.jpg" alt="" /></p>
<p><img src="media/16016274752497/16016307437464.jpg" alt="" /></p>
<p>\(K_{A}=1, e_{A}=0, C_{A}=0\)<br />
\(k_{B}=2, e_{B}=1, C_{B}=1\)<br />
\(k_{C}=4, e_{C}=2, C_{C}=1/3\)<br />
\(k_{D}=4, e_{D}=2, C_{D}=1/3\)<br />
\(k_{E}=3, e_{E}=0, C_{E}=0\)<br />
\(k_{F}=1, e_{F}=0, C_{F}=0\)<br />
\(k_{G}=1, e_{G}=0, C_{G}=0\)<br />
\(k_{H}=2, e_{H}=1, C_{H}=1\)<br />
avg. clustering: C= (1+1/3+1/3+1)/8=1/3</p>
<h3><a id="connected-components" class="anchor" aria-hidden="true" href="#connected-components"><span class="octicon octicon-link"></span></a>Connected components</h3>
<p><strong>Connectivity</strong><br />
图当中最大的可连接的component：能够通过path链接的任意两个几点的最大的集合；<br />
如何找到图当中的connect components，从图中随机节点开始，按广度优先策略遍历，标记遍历过的节点，如果，所有的节点均被遍历，那么这个未connected component， 否则从未遍历的节点中随机开始，重复广度优先策略遍历；</p>
<p><img src="media/16016274752497/16016314705304.jpg" alt="" /></p>
<h2><a id="%E6%8F%8F%E8%BF%B0%E5%AE%9E%E9%99%85%E4%B8%AD%E7%9A%84%E5%9B%BE%EF%BC%9Amsn-messenger" class="anchor" aria-hidden="true" href="#%E6%8F%8F%E8%BF%B0%E5%AE%9E%E9%99%85%E4%B8%AD%E7%9A%84%E5%9B%BE%EF%BC%9Amsn-messenger"><span class="octicon octicon-link"></span></a>描述实际中的图：MSN Messenger</h2>
<p>msn一个月的相关的数据，如下：<br />
<img src="media/16016274752497/16016320300517.jpg" alt="" /></p>
<h3><a id="degree-distribution" class="anchor" aria-hidden="true" href="#degree-distribution"><span class="octicon octicon-link"></span></a>Degree Distribution</h3>
<p><img src="media/16016274752497/16016372202411.jpg" alt="" /><br />
x坐标log之后：<br />
<img src="media/16016274752497/16016372583523.jpg" alt="" /><br />
可见大部分的节点degress在个位数。</p>
<h3><a id="clustering" class="anchor" aria-hidden="true" href="#clustering"><span class="octicon octicon-link"></span></a>Clustering</h3>
<p>将所有的节点的k与c绘制在如下图中，整个graph的avg culstering coefficient约为0.1140<br />
<img src="media/16016274752497/16016374053942.jpg" alt="" /></p>
<h3><a id="connected-components" class="anchor" aria-hidden="true" href="#connected-components"><span class="octicon octicon-link"></span></a>Connected Components</h3>
<p><img src="media/16016274752497/16016375024548.jpg" alt="" /></p>
<h3><a id="diameter" class="anchor" aria-hidden="true" href="#diameter"><span class="octicon octicon-link"></span></a>Diameter</h3>
<p>msn的graph中平均path length为6.6， 90% 的节点能够触及在8个链接后触及到另一节点；<br />
<img src="media/16016274752497/16016375894524.jpg" alt="" /></p>
<h3><a id="%E5%9B%BE%E7%9A%84%E6%A0%B8%E5%BF%83%E5%B1%9E%E6%80%A7%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%EF%BC%9F" class="anchor" aria-hidden="true" href="#%E5%9B%BE%E7%9A%84%E6%A0%B8%E5%BF%83%E5%B1%9E%E6%80%A7%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%EF%BC%9F"><span class="octicon octicon-link"></span></a>图的核心属性如何使用？</h3>
<p>这些graph的属性是意外的还是在我们本身预料之中？<br />
<img src="media/16016274752497/16016377096309.jpg" alt="" /></p>
<p><strong>PPI Network</strong></p>
<p><img src="media/16016274752497/16016378153380.jpg" alt="" /></p>
<h2><a id="random-graph-model" class="anchor" aria-hidden="true" href="#random-graph-model"><span class="octicon octicon-link"></span></a>Random Graph Model</h2>
<h3><a id="simplest-model-of-graph" class="anchor" aria-hidden="true" href="#simplest-model-of-graph"><span class="octicon octicon-link"></span></a>Simplest Model of Graph</h3>
<p><strong>ER Random Graphs</strong><br />
两个变种：</p>
<ol>
<li>\(G_{np}\)： n个节点的无向图，其中每一条边是概率为p的独立同分布；</li>
<li>\(G_{nm}\): n个节点的无向图，其中m个边均匀随机生成；</li>
</ol>
<p>需要说明的是，n, p 无法唯一地的决定graph，如下图，相同的n,p下， 我们有不同的图：<br />
<img src="media/16016274752497/16016380802516.jpg" alt="" /></p>
<h4><a id="degree-distribution-of-g-np" class="anchor" aria-hidden="true" href="#degree-distribution-of-g-np"><span class="octicon octicon-link"></span></a>Degree Distribution of \(G_{np}\)</h4>
<p>假定\(P{(k)}\)表示度为k在所有节点中的占比， 则</p>
\[P_{(k)}= C_{n-1}^{k} p^{k}(1-p)^{n-1-k}
\]
<p>很明显的binomial distribution, 所以均值、方差为：</p>
\[k_mean = p(n-1)
\]
\[\sigma^2 = p(1-p)(n-1)
\]
<p>标准差率为：\(\frac{\sigma}{k_mean} \approx \frac{1}{(n-1)^{0.5}}\)，当图无限大的时候，则标准差为0， 所有的节点都为\(k_mean\)。</p>
<h4><a id="clustering-coefficient-of-g-np" class="anchor" aria-hidden="true" href="#clustering-coefficient-of-g-np"><span class="octicon octicon-link"></span></a>Clustering Coefficient of \(G_{np}\)</h4>
<p>已知\(C_{i}=\frac{2e_{i}}{k_{i}(k_{i}-1)}\), 在\(G_{np}\)，边为概率为p的独立同步分， 其中\(E[e_{i}] = p\frac{k_{i}(k_{i}-1)}{2}\), 故\(E[C_i] = \frac{pk_i(k_i - 1)}{k_i(k_i -1)} = p = \frac{k_{avg}}{n-1} \approx \frac{k_{avg}}{n}\)</p>
<p><strong>Expansion</strong></p>
<p>定义\(\alpha\): 如果一个graph的任意的子集S,子集中边的条数大于alpha乘以子集或者graph去除子集之后的节点数量， Expansion通常用来衡量图的lu'bang'xing：</p>
\[\alpha = \mathop{\min}_{S \subseteqq}{\frac{\# edges\  leaving\ S} {min(|s|, |V \ S|)}}
\]
<p><img src="media/16016274752497/16016422849257.jpg" alt="" /><br />
这张ppt没理解清楚，<br />
<img src="media/16016274752497/16016429720141.jpg" alt="" /><br />
在\(G_{np}\)中，n*p为常数，所以avg deg k也为常数：<br />
<img src="media/16016274752497/16016430308075.jpg" alt="" /></p>
<h4><a id="connected-components" class="anchor" aria-hidden="true" href="#connected-components"><span class="octicon octicon-link"></span></a>Connected Components</h4>
<p>\(G_{np}，其中n = 100000, k=p(n-1)=0.5...3\)，Largest CC中节点占图中所有节点的比例<br />
<img src="media/16016274752497/16016433773242.jpg" alt="" /></p>
<h2><a id="random-graph-model-vs-msn" class="anchor" aria-hidden="true" href="#random-graph-model-vs-msn"><span class="octicon octicon-link"></span></a>Random Graph Model vs. MSN</h2>
<p>在Random Graph Model 和实际的MNS的4个核心属性对比：<br />
<img src="media/16016274752497/16016435436504.jpg" alt="" /></p>
<p>真实网络和Random Graph类似吗 ？</p>
<ul>
<li>Giant Connected component: yes</li>
<li>Average path length: yes</li>
<li>Clustering Coefficient: No</li>
<li>Degree Distribution: No</li>
</ul>
<h2><a id="the-small-world-model%E8%83%BD%E5%90%8C%E6%97%B6%E4%BF%9D%E8%AF%81-high-clustering%E4%B8%94%E7%9F%ADpath%E7%9A%84%E5%9B%BE%E5%90%97%EF%BC%9F" class="anchor" aria-hidden="true" href="#the-small-world-model%E8%83%BD%E5%90%8C%E6%97%B6%E4%BF%9D%E8%AF%81-high-clustering%E4%B8%94%E7%9F%ADpath%E7%9A%84%E5%9B%BE%E5%90%97%EF%BC%9F"><span class="octicon octicon-link"></span></a>The Small-World Model--能同时保证high clustering且短path的图吗？</h2>
<p><img src="media/16016274752497/16016440381977.jpg" alt="" /></p>
<p>回顾下前面MSN network，clustering coef为0.11， 而\(G_{np}的clustering coef为8*10^{-8}\)。<br />
另外一个例子， IMDB数据集、Electrical power grid, Network of nerons中：<br />
<img src="media/16016274752497/16016442022709.jpg" alt="" /><br />
其中h：average shortest path length， C: avg clustering coefficient， random，是保证相同avg degree，相同节点下的图的情况。</p>
<p><img src="media/16016274752497/16016445880044.jpg" alt="" /></p>
<p>下图左边：高clustering coefficient： 朋友的朋友是我的朋友；</p>
<p><img src="media/16016274752497/16016446507292.jpg" alt="" /></p>
<p>Small-World同时保证high cluster and low diameter；<br />
如下图，从high clustering/high diameter, 到low clustering/low diameter， 增加随机性（p变大）: 即随机的将一条边的另一个端点连接到任意较远的节点上，这样可以保持high clustering，low diameter；<br />
<img src="media/16016274752497/16016448836939.jpg" alt="" /><br />
下图中的p区域保证保证high clustering 和low path length：<br />
<img src="media/16016274752497/16016449171942.jpg" alt="" /></p>
<h2><a id="kronecker-graph-model-generating-large-realistic-graphs" class="anchor" aria-hidden="true" href="#kronecker-graph-model-generating-large-realistic-graphs"><span class="octicon octicon-link"></span></a>Kronecker Graph Model: Generating large realistic graphs</h2>
<p>递归的graph的生成: Self-similarity</p>
<p><img src="media/16016274752497/16016462617663.jpg" alt="" /></p>
<p><strong>Kronecker Produce</strong>是一种生成self-similar矩阵的方法：<br />
<img src="media/16016274752497/16016463025641.jpg" alt="" /><br />
Kronecker Product 定义如下：<br />
<img src="media/16016274752497/16016463141556.jpg" alt="" /><br />
举个例子：</p>
<p><img src="media/16016274752497/16016463666640.jpg" alt="" /></p>
<ul>
<li>构建一个\(N_1 * N_1\)的初始概率矩阵；</li>
<li>计算k阶Kronecker 矩阵；</li>
<li>遍历k阶矩阵，按\(p_{uv}\)构建edge(u, v)链接<br />
<img src="media/16016274752497/16016466109703.jpg" alt="" /><br />
如上图最后， 需要模拟\(n^2\)次，耗时太高， 是否有更高效方法(利用其递归结构)？</li>
</ul>
<p><img src="media/16016274752497/16016470904240.jpg" alt="" /></p>
<p>真实网络与Kronecker网络很相似， 右上角为其初始矩阵：</p>
<p><img src="media/16016274752497/16016471370705.jpg" alt="" /></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/10/02</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%9B%BE%E8%AE%A1%E7%AE%97.html'>图计算</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>小石头的码疯窝</h1>
                <div class="site-des">记录一些学习笔记</div>
                <div class="social">











  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html"><strong>推荐系统</strong></a>
        
            <a href="TensorFlow.html"><strong>TensorFlow</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0.html"><strong>机器学习平台</strong></a>
        
            <a href="%E5%9B%BE%E8%AE%A1%E7%AE%97.html"><strong>图计算</strong></a>
        
            <a href="pytorch.html"><strong>pytorch</strong></a>
        
            <a href="%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F.html"><strong>分布式系统</strong></a>
        
            <a href="mlops.html"><strong>mlops</strong></a>
        
            <a href="paddle.html"><strong>paddle</strong></a>
        
            <a href="%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8.html"><strong>参数服务器</strong></a>
        
            <a href="GAN.html"><strong>GAN</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="16533711069508.html">网易云音乐机器学习平台实践</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="16490569390259.html">模型生产环境中的反馈与数据回流</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="16482627913984.html">mlops之监控与数据回流</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="16371350579612.html">机器学习平台在云音乐的持续实践</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="16268581441013.html">为机器学习量身定做的ops工具：cml & dvc</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>



  














<style type="text/css">
figure{margin: 0;padding: 0;}
figcaption{text-align:center;}

/* PrismJS 1.14.0
 http://prismjs.com/download.html#themes=prism&languages=markup+css+clike+javascript */
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
    color: black;
    background: none;
    text-shadow: 0 1px white;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
    
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
    
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
    text-shadow: none;
    background:#b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
    text-shadow: none;
    background: #b3d4fc;
}

@media print {
    code[class*="language-"],
    pre[class*="language-"] {
        text-shadow: none;
    }
}

/* Code blocks */
pre[class*="language-"] {
    padding: 1em;
    margin: .5em 0;
    overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
    background: #F7F7F7;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
    padding: .1em;
    border-radius: .3em;
    white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
    color: slategray;
}

.token.punctuation {
    color: #999;
}

.namespace {
    opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
    color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
    color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
    color: #9a6e3a;
    background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
    color: #07a;
}

.token.function,
.token.class-name {
    color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
    color: #e90;
}

.token.important,
.token.bold {
    font-weight: bold;
}
.token.italic {
    font-style: italic;
}

.token.entity {
    cursor: help;
}


pre[class*="language-"].line-numbers {
    position: relative;
    padding-left: 3.8em;
    counter-reset: linenumber;
}

pre[class*="language-"].line-numbers > code {
    position: relative;
    white-space: inherit;
}

.line-numbers .line-numbers-rows {
    position: absolute;
    pointer-events: none;
    top: 0;
    font-size: 100%;
    left: -3.8em;
    width: 3em; /* works for line-numbers below 1000 lines */
    letter-spacing: -1px;
    border-right: 1px solid #999;

    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;

}

    .line-numbers-rows > span {
        pointer-events: none;
        display: block;
        counter-increment: linenumber;
    }

        .line-numbers-rows > span:before {
            content: counter(linenumber);
            color: #999;
            display: block;
            padding-right: 0.8em;
            text-align: right;
        }

</style>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>



  </body>
</html>
