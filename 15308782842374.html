<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  炼丹师读源码之PaddlePaddle底层计算架构设计 - 
  
  </title>
 <meta name="description" content="">
 <link href="atom.xml" rel="alternate" title="" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />

    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
    <script src="asset/highlightjs/highlight.pack.js"></script>
    <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
    <script>hljs.initHighlightingOnLoad();</script>
    
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>

<div id="header">
    <h1><a href="index.html"></a></h1>
</div>

</nav>
        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; </span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
      <li><a href="index.html">Home</a></li>
      
        <li class="divider"></li>
        <li><label>推荐系统</label></li>

          
            <li><a title="推荐系统的禅与道" href="15951226773958.html">推荐系统的禅与道</a></li>
          
            <li><a title="推荐系统灌水文之如何理解Item" href="15763400595374.html">推荐系统灌水文之如何理解Item</a></li>
          

      
        <li class="divider"></li>
        <li><label>TensorFlow</label></li>

          
            <li><a title="TensorFlow Sparse的一个优化" href="15768353446694.html">TensorFlow Sparse的一个优化</a></li>
          
            <li><a title="基础知识背景" href="15768070172080.html">基础知识背景</a></li>
          
            <li><a title="TensorFlow优化" href="15744799610685.html">TensorFlow优化</a></li>
          
            <li><a title="GDG上海 2019.10.13分享内容准备" href="15673444950441.html">GDG上海 2019.10.13分享内容准备</a></li>
          
            <li><a title="Dive Into TensorFlow" href="15659603996762.html">Dive Into TensorFlow</a></li>
          
            <li><a title="Maybe Best Practice With Sparse Machine Learning In TensorFlow" href="15430699092916.html">Maybe Best Practice With Sparse Machine Learning In TensorFlow</a></li>
          

      
        <li class="divider"></li>
        <li><label>机器学习平台</label></li>

          
            <li><a title="---" href="16533711069508.html">---</a></li>
          
            <li><a title="机器学习平台在云音乐的持续实践" href="16371350579612.html">机器学习平台在云音乐的持续实践</a></li>
          
            <li><a title="泛模型管理与分发" href="16076083666793.html">泛模型管理与分发</a></li>
          
            <li><a title="机器学习与容器化平台" href="15861773530303.html">机器学习与容器化平台</a></li>
          
            <li><a title="机器学习工程实践" href="15648106167930.html">机器学习工程实践</a></li>
          

      
        <li class="divider"></li>
        <li><label>图计算</label></li>

          
            <li><a title="直播场景PGL落地" href="16078481811620.html">直播场景PGL落地</a></li>
          
            <li><a title="Graph Neural Networks" href="16077533959046.html">Graph Neural Networks</a></li>
          
            <li><a title="graph representation learning" href="16077533959085.html">graph representation learning</a></li>
          
            <li><a title="Spectral Clustering" href="16029890553575.html">Spectral Clustering</a></li>
          
            <li><a title="Community structure in networks" href="16019601035128.html">Community structure in networks</a></li>
          
            <li><a title="Motifs and structural roles in networks" href="16017096445579.html">Motifs and structural roles in networks</a></li>
          
            <li><a title="Properties of Networks and Random Graph Model" href="16016274752497.html">Properties of Networks and Random Graph Model</a></li>
          

      
        <li class="divider"></li>
        <li><label>pytorch</label></li>

          
            <li><a title="pytorch 环境安装及配置" href="15808187045354.html">pytorch 环境安装及配置</a></li>
          

      
        <li class="divider"></li>
        <li><label>分布式系统</label></li>

          
            <li><a title="分布式一致性" href="15860748439491.html">分布式一致性</a></li>
          
            <li><a title="raft 参考文章" href="15854050053037.html">raft 参考文章</a></li>
          
            <li><a title="分布式系统书籍" href="15853877486542.html">分布式系统书籍</a></li>
          
            <li><a title="容灾【备份】" href="15848127872361.html">容灾【备份】</a></li>
          
            <li><a title="容灾【备份】" href="15848127093419.html">容灾【备份】</a></li>
          

      
        <li class="divider"></li>
        <li><label>mlops</label></li>

          
            <li><a title="模型生产环境中的反馈与数据回流" href="16490569390259.html">模型生产环境中的反馈与数据回流</a></li>
          
            <li><a title="mlops之监控与数据回流" href="16482627913984.html">mlops之监控与数据回流</a></li>
          
            <li><a title="为机器学习量身定做的ops工具：cml & dvc" href="16268581441013.html">为机器学习量身定做的ops工具：cml & dvc</a></li>
          
            <li><a title="[toc]" href="16003962131575.html">[toc]</a></li>
          

      
        <li class="divider"></li>
        <li><label>paddle</label></li>

          
            <li><a title="直播场景PGL落地" href="16078655202819.html">直播场景PGL落地</a></li>
          

      
        <li class="divider"></li>
        <li><label>参数服务器</label></li>

          
            <li><a title="ps-lite" href="15836784836219.html">ps-lite</a></li>
          
            <li><a title="ps-lite再一次研读源码" href="15536014868367.html">ps-lite再一次研读源码</a></li>
          

      
        <li class="divider"></li>
        <li><label>GAN</label></li>

          
            <li><a title="深度解析为什么GAN能是Structured Learning的一种解决方案" href="Introduction%20to%20GAN.html">深度解析为什么GAN能是Structured Learning的一种解决方案</a></li>
          

      
      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>

        <section id="main-content" role="main" class="scroll-container">

          <div class="row">
            <div class="large-3 medium-3 columns">
              <div class="hide-for-small">
                <div class="sidebar">
                <nav>
                  <ul id="side-nav" class="side-nav">

                    
                      <li class="side-title"><span>推荐系统</span></li>
                        
                          <li><a title="推荐系统的禅与道" href="15951226773958.html">推荐系统的禅与道</a></li>
                        
                          <li><a title="推荐系统灌水文之如何理解Item" href="15763400595374.html">推荐系统灌水文之如何理解Item</a></li>
                        

                    
                      <li class="side-title"><span>TensorFlow</span></li>
                        
                          <li><a title="TensorFlow Sparse的一个优化" href="15768353446694.html">TensorFlow Sparse的一个优化</a></li>
                        
                          <li><a title="基础知识背景" href="15768070172080.html">基础知识背景</a></li>
                        
                          <li><a title="TensorFlow优化" href="15744799610685.html">TensorFlow优化</a></li>
                        
                          <li><a title="GDG上海 2019.10.13分享内容准备" href="15673444950441.html">GDG上海 2019.10.13分享内容准备</a></li>
                        
                          <li><a title="Dive Into TensorFlow" href="15659603996762.html">Dive Into TensorFlow</a></li>
                        
                          <li><a title="Maybe Best Practice With Sparse Machine Learning In TensorFlow" href="15430699092916.html">Maybe Best Practice With Sparse Machine Learning In TensorFlow</a></li>
                        

                    
                      <li class="side-title"><span>机器学习平台</span></li>
                        
                          <li><a title="---" href="16533711069508.html">---</a></li>
                        
                          <li><a title="机器学习平台在云音乐的持续实践" href="16371350579612.html">机器学习平台在云音乐的持续实践</a></li>
                        
                          <li><a title="泛模型管理与分发" href="16076083666793.html">泛模型管理与分发</a></li>
                        
                          <li><a title="机器学习与容器化平台" href="15861773530303.html">机器学习与容器化平台</a></li>
                        
                          <li><a title="机器学习工程实践" href="15648106167930.html">机器学习工程实践</a></li>
                        

                    
                      <li class="side-title"><span>图计算</span></li>
                        
                          <li><a title="直播场景PGL落地" href="16078481811620.html">直播场景PGL落地</a></li>
                        
                          <li><a title="Graph Neural Networks" href="16077533959046.html">Graph Neural Networks</a></li>
                        
                          <li><a title="graph representation learning" href="16077533959085.html">graph representation learning</a></li>
                        
                          <li><a title="Spectral Clustering" href="16029890553575.html">Spectral Clustering</a></li>
                        
                          <li><a title="Community structure in networks" href="16019601035128.html">Community structure in networks</a></li>
                        
                          <li><a title="Motifs and structural roles in networks" href="16017096445579.html">Motifs and structural roles in networks</a></li>
                        
                          <li><a title="Properties of Networks and Random Graph Model" href="16016274752497.html">Properties of Networks and Random Graph Model</a></li>
                        

                    
                      <li class="side-title"><span>pytorch</span></li>
                        
                          <li><a title="pytorch 环境安装及配置" href="15808187045354.html">pytorch 环境安装及配置</a></li>
                        

                    
                      <li class="side-title"><span>分布式系统</span></li>
                        
                          <li><a title="分布式一致性" href="15860748439491.html">分布式一致性</a></li>
                        
                          <li><a title="raft 参考文章" href="15854050053037.html">raft 参考文章</a></li>
                        
                          <li><a title="分布式系统书籍" href="15853877486542.html">分布式系统书籍</a></li>
                        
                          <li><a title="容灾【备份】" href="15848127872361.html">容灾【备份】</a></li>
                        
                          <li><a title="容灾【备份】" href="15848127093419.html">容灾【备份】</a></li>
                        

                    
                      <li class="side-title"><span>mlops</span></li>
                        
                          <li><a title="模型生产环境中的反馈与数据回流" href="16490569390259.html">模型生产环境中的反馈与数据回流</a></li>
                        
                          <li><a title="mlops之监控与数据回流" href="16482627913984.html">mlops之监控与数据回流</a></li>
                        
                          <li><a title="为机器学习量身定做的ops工具：cml & dvc" href="16268581441013.html">为机器学习量身定做的ops工具：cml & dvc</a></li>
                        
                          <li><a title="[toc]" href="16003962131575.html">[toc]</a></li>
                        

                    
                      <li class="side-title"><span>paddle</span></li>
                        
                          <li><a title="直播场景PGL落地" href="16078655202819.html">直播场景PGL落地</a></li>
                        

                    
                      <li class="side-title"><span>参数服务器</span></li>
                        
                          <li><a title="ps-lite" href="15836784836219.html">ps-lite</a></li>
                        
                          <li><a title="ps-lite再一次研读源码" href="15536014868367.html">ps-lite再一次研读源码</a></li>
                        

                    
                      <li class="side-title"><span>GAN</span></li>
                        
                          <li><a title="深度解析为什么GAN能是Structured Learning的一种解决方案" href="Introduction%20to%20GAN.html">深度解析为什么GAN能是Structured Learning的一种解决方案</a></li>
                        

                    
                  </ul>
                </nav>
                </div>
              </div>
            </div>
            <div class="large-9 medium-9 columns">

 <div class="markdown-body">
<h1>炼丹师读源码之PaddlePaddle底层计算架构设计</h1>

<ul>
<li><a href="#%E6%95%B0%E5%80%BC%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1">数值架构设计</a>
<ul>
<li><a href="#paddle-v2">Paddle V2</a></li>
<li><a href="#paddlefluid">PaddleFluid</a>
<ul>
<li><a href="#tensor%E6%9B%BF%E4%BB%A3matrix">Tensor替代Matrix</a></li>
</ul>
</li>
</ul>
</li>
</ul>

<h2><a id="%E6%95%B0%E5%80%BC%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1" class="anchor" aria-hidden="true" href="#%E6%95%B0%E5%80%BC%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="octicon octicon-link"></span></a>数值架构设计</h2>
<h3><a id="paddle-v2" class="anchor" aria-hidden="true" href="#paddle-v2"><span class="octicon octicon-link"></span></a>Paddle V2</h3>
<h3><a id="paddlefluid" class="anchor" aria-hidden="true" href="#paddlefluid"><span class="octicon octicon-link"></span></a>PaddleFluid</h3>
<h4><a id="tensor%E6%9B%BF%E4%BB%A3matrix" class="anchor" aria-hidden="true" href="#tensor%E6%9B%BF%E4%BB%A3matrix"><span class="octicon octicon-link"></span></a>Tensor替代Matrix</h4>
<p><strong>Placeholder与PlaceholderImpl</strong></p>
<pre><code>struct Placeholder {
    virtual ~Placeholder() = default;
    virtual void* ptr() const = 0;
    virtual size_t size() const = 0;
    virtual std::type_index type() const = 0;
    // place.h有相关定义
    virtual platform::Place place() const = 0;
    virtual void set_type(std::type_index type) = 0;
    virtual void set_place(platform::Place place) = 0;
};

struct PlaceholderImpl : public Placeholder {
    PlaceholderImpl(Place place, size_t size, std::type_index type)
        : ptr_(static_cast&lt;uint8_t*&gt;(memory::Alloc(place, size)),
            memory::PODDeleter&lt;uint8_t, Place&gt;(place)),
        place_(place),
        size_(size),
        type_(type) {
    PADDLE_ENFORCE_NOT_NULL(ptr_, &quot;Insufficient %s memory to allocation.&quot;,
                            (is_cpu_place(place_) ? &quot;CPU&quot; : &quot;GPU&quot;));
    }

    virtual size_t size() const { return size_; }
    virtual platform::Place place() const { return place_; }
    virtual void* ptr() const { return static_cast&lt;void*&gt;(ptr_.get()); }
    virtual std::type_index type() const { return type_; }
    virtual void set_type(std::type_index type) { type_ = type; }
    virtual void set_place(platform::Place place) { place_ = place; }

    /*! the pointer of memory block. */
    std::unique_ptr&lt;uint8_t, memory::PODDeleter&lt;uint8_t, Place&gt;&gt; ptr_;

    /*! the place of memory block. */
    platform::Place place_;

    /*! the size of memory block. */
    size_t size_;

    /* the current type of memory */
    std::type_index type_;
};
</code></pre>
<p><strong>DDim</strong></p>
<pre><code>struct DDim {
    // variant 联合体，multi type， single value， 这里的Dim&lt;0&gt;可以在下面看dim.h里结构体Dim的定义, 这里最多支持rank为9的维度，也就是说9维变量
    typedef boost::variant&lt;Dim&lt;0&gt;, Dim&lt;1&gt;, Dim&lt;2&gt;, Dim&lt;3&gt;, Dim&lt;4&gt;, Dim&lt;5&gt;, Dim&lt;6&gt;,
                            Dim&lt;7&gt;, Dim&lt;8&gt;, Dim&lt;9&gt;&gt;
        DDimVar;
    DDimVar var;

    DDim() : var(Dim&lt;1&gt;()) {}

    template &lt;int D&gt;
    explicit DDim(const Dim&lt;D&gt;&amp; in) : var(in) {}

    /*implicit*/ DDim(std::initializer_list&lt;int64_t&gt; init_list);

    template &lt;int D&gt;
    DDim&amp; operator=(const Dim&lt;D&gt;&amp; in) {
        var = in;
        return *this;
    }

    int64_t&amp; operator[](int idx);
    int64_t operator[](int idx) const;

    template &lt;typename Visitor&gt;
    typename Visitor::result_type apply_visitor(Visitor&amp; visitor) {
        return var.apply_visitor(visitor);
    }

    template &lt;typename Visitor&gt;
    typename Visitor::result_type apply_visitor(Visitor&amp; visitor) const {
        return var.apply_visitor(visitor);
    }

    DDimVar getVar() { return var; }

    bool operator==(DDim d) const;

    bool operator!=(DDim d) const;

    DDim operator+(DDim d) const;

    DDim operator*(DDim d) const;

    int size() const;
    };

// 从vector来创建DDim
DDim make_ddim(const std::vector&lt;int64_t&gt;&amp; dims);
Dim make_ddim(const std::vector&lt;int&gt;&amp; dims);

// 模板， i为维度
template &lt;int i&gt;
struct Dim {
    static constexpr int dimensions = i;

    template &lt;typename... Args&gt;
    HOSTDEVICE Dim(int64_t _head, Args... _tail) : head(_head), tail(_tail...) {
        static_assert(sizeof...(_tail) == i - 1,
                    &quot;Dim initialized with the wrong number of parameters&quot;);
    }

    HOSTDEVICE
    Dim(int64_t _head, const Dim&lt;i - 1&gt;&amp; _tail) : head(_head), tail(_tail) {}

    HOSTDEVICE
    Dim() : head(0), tail() {}

    /** Construct a Dim from a linear index and size.  Uses Fortran order
    * indexing. */
    HOSTDEVICE
    Dim(int64_t idx, const Dim&lt;i&gt;&amp; size)
        : head(idx % size.head), tail(idx / size.head, size.tail) {}

    /** Construct a Dim with each dimension set to the given index */
    HOSTDEVICE
    Dim(int64_t idx) : head(idx), tail(idx) {}

    HOSTDEVICE
    bool operator==(const Dim&lt;i&gt;&amp; o) const {
        return (head == o.head) &amp;&amp; (tail == o.tail);
    }

    HOSTDEVICE
    bool operator!=(const Dim&lt;i&gt;&amp; o) const { return !(*this == o); }

    HOSTDEVICE
    int64_t&amp; operator[](int idx);
    HOSTDEVICE
    int64_t operator[](int idx) const;

    HOST std::string to_string() const;

    int64_t head;
    Dim&lt;i - 1&gt; tail;
    };
</code></pre>
<p><strong>DataLayout</strong><br />
DataLayout用于支持不同的DataLayout,如何NHWC、HCHW，KAnyLayout、KMKLDNN等等；</p>
<pre><code>enum class DataLayout {
    kNHWC = 0,
    kNCHW = 1,
    kAnyLayout = 2,
    kMKLDNN = 3,  // all layouts supported by MKLDNN internally
    };
</code></pre>
<p><strong>Place</strong></p>
<pre><code>struct CPUPlace {
  // WORKAROUND: for some reason, omitting this constructor
  // causes errors with boost 1.59 and OSX
  CPUPlace() {}

  // needed for variant equality comparison
  inline bool operator==(const CPUPlace &amp;) const { return true; }
  inline bool operator!=(const CPUPlace &amp;) const { return false; }
};

struct CUDAPlace {
  CUDAPlace() : CUDAPlace(0) {}
  explicit CUDAPlace(int d) : device(d) {}

  inline int GetDeviceId() const { return device; }
  // needed for variant equality comparison
  inline bool operator==(const CUDAPlace &amp;o) const {
      return device == o.device;
  }
  inline bool operator!=(const CUDAPlace &amp;o) const { return !(*this == o); }

  int device;
};

struct CUDAPinnedPlace {
  CUDAPinnedPlace() {}

  // needed for variant equality comparison
  inline bool operator==(const CUDAPinnedPlace &amp;) const { return true; }
  inline bool operator!=(const CUDAPinnedPlace &amp;) const { return false; }
};
// visitor设计模式与boost static_visitor 模式 见 http://bitdewy.github.io/blog/2013/07/15/static-visitor/
struct IsCUDAPlace : public boost::static_visitor&lt;bool&gt; {
  bool operator()(const CPUPlace &amp;) const { return false; }
  bool operator()(const CUDAPlace &amp;gpu) const { return true; }
  bool operator()(const CUDAPinnedPlace &amp;) const { return false; }
};

struct IsCPUPlace : public boost::static_visitor&lt;bool&gt; {
  bool operator()(const CPUPlace &amp;cpu) const { return true; }
  bool operator()(const CUDAPlace &amp;) const { return false; }
  bool operator()(const CUDAPinnedPlace &amp;) const { return false; }
};

struct IsCUDAPinnedPlace : public boost::static_visitor&lt;bool&gt; {
  bool operator()(const CPUPlace &amp;) const { return false; }
  bool operator()(const CUDAPlace &amp;) const { return false; }
  bool operator()(const CUDAPinnedPlace &amp;cuda_pinned) const { return true; }
};

typedef boost::variant&lt;CUDAPlace, CPUPlace, CUDAPinnedPlace&gt; Place;

using PlaceList = std::vector&lt;Place&gt;;

void set_place(const Place &amp;);
const Place &amp;get_place();

const CUDAPlace default_gpu();
const CPUPlace default_cpu();
const CUDAPinnedPlace default_cuda_pinned();

bool is_gpu_place(const Place &amp;);
bool is_cpu_place(const Place &amp;);
bool is_cuda_pinned_place(const Place &amp;);
bool places_are_same_class(const Place &amp;, const Place &amp;);
bool is_same_place(const Place &amp;, const Place &amp;);

struct PlaceHash {
std::size_t operator()(const Place &amp;p) const {
    constexpr size_t num_dev_bits = 4;
    std::hash&lt;int&gt; ihash;
    size_t dev_id = 0;
    if (is_gpu_place(p)) {
      // 如果是gpu设备，则拿到对应的device id
      dev_id = boost::get&lt;CUDAPlace&gt;(p).device;
    }
    return ihash(dev_id &lt;&lt; num_dev_bits | p.which());
}
};

std::ostream &amp;operator&lt;&lt;(std::ostream &amp;, const Place &amp;);

template &lt;typename Visitor&gt;
struct PlaceVisitorWrapper
    : public boost::static_visitor&lt;typename Visitor::result_type&gt; {
const Visitor &amp;visitor_;
explicit PlaceVisitorWrapper(const Visitor &amp;visitor) : visitor_(visitor) {}

typename Visitor::result_type operator()(const CPUPlace &amp;cpu) const {
    return visitor_(cpu);
}

typename Visitor::result_type operator()(const CUDAPlace &amp;cuda) const {
#ifdef PADDLE_WITH_CUDA
    return visitor_(cuda);
#else
    PADDLE_THROW(&quot;Paddle is not compiled with CUDA. Cannot visit cuda device&quot;);
    return typename Visitor::result_type();
#endif
}

typename Visitor::result_type operator()(
    const CUDAPinnedPlace &amp;cuda_pinned) const {
#ifdef PADDLE_WITH_CUDA
    return visitor_(cuda_pinned);
#else
    PADDLE_THROW(&quot;Paddle is not compiled with CUDA. Cannot visit cuda_pinned&quot;);
    return typename Visitor::result_type();
#endif
}
};

template &lt;typename Visitor&gt;
typename Visitor::result_type VisitPlace(const Place &amp;place,
                                        const Visitor &amp;visitor) {
return boost::apply_visitor(PlaceVisitorWrapper&lt;Visitor&gt;(visitor), place);
}
</code></pre>
<p><strong>Tensor()</strong><br />
Tensor构造函数，包括参数为Place的构造函数，配置holder_为对应place。</p>
<pre><code>Tensor() : offset_(0) {}

/*! Constructor with place should only be used in pybind. */
explicit Tensor(const platform::Place&amp; place) : offset_(0) {
    holder_-&gt;set_place(place);
}
</code></pre>
<p><strong>data()</strong></p>
<p>data()返回内存块对应的指针：</p>
<pre><code>template &lt;typename T&gt;
inline const T* Tensor::data() const {
check_memory_size();
PADDLE_ENFORCE(std::is_same&lt;T, void&gt;::value ||
                    holder_-&gt;type() == std::type_index(typeid(T)),
                &quot;Tensor holds the wrong type, it holds %s&quot;,
                this-&gt;holder_-&gt;type().name());
// typedef unsigned long int uintptr_t;  
return reinterpret_cast&lt;const T*&gt;(
    reinterpret_cast&lt;uintptr_t&gt;(holder_-&gt;ptr()) + offset_);
}

template &lt;typename T&gt;
inline const T* Tensor::data() const {
check_memory_size();
PADDLE_ENFORCE(std::is_same&lt;T, void&gt;::value ||
                    holder_-&gt;type() == std::type_index(typeid(T)),
                &quot;Tensor holds the wrong type, it holds %s&quot;,
                this-&gt;holder_-&gt;type().name());

return reinterpret_cast&lt;const T*&gt;(
    reinterpret_cast&lt;uintptr_t&gt;(holder_-&gt;ptr()) + offset_);
}
</code></pre>
<p><strong>mutable_data()</strong></p>
<pre><code>void* Tensor::mutable_data(platform::Place place, std::type_index type) {
    if (holder_ != nullptr) {
        holder_-&gt;set_type(type);
    }
    PADDLE_ENFORCE_GE(numel(), 0,
                        &quot;When calling this method, the Tensor's numel must be &quot;
                        &quot;equal or larger than zero. &quot;
                        &quot;Please check Tensor::Resize has been called first.&quot;);
    int64_t size = numel() * SizeOfType(type);
    /* some versions of boost::variant don't have operator!= */
    if (holder_ == nullptr || !(holder_-&gt;place() == place) ||
        holder_-&gt;size() &lt; size + offset_) {
        if (platform::is_cpu_place(place)) {
        holder_.reset(new PlaceholderImpl&lt;platform::CPUPlace&gt;(
            boost::get&lt;platform::CPUPlace&gt;(place), size, type));
        } else if (platform::is_gpu_place(place) ||
                platform::is_cuda_pinned_place(place)) {
    #ifndef PADDLE_WITH_CUDA
        PADDLE_THROW(
            &quot;CUDAPlace or CUDAPinnedPlace is not supported in CPU-only mode.&quot;);
        }
    #else
        if (platform::is_gpu_place(place)) {
            holder_.reset(new PlaceholderImpl&lt;platform::CUDAPlace&gt;(
                boost::get&lt;platform::CUDAPlace&gt;(place), size, type));
        } else if (platform::is_cuda_pinned_place(place)) {
            holder_.reset(new PlaceholderImpl&lt;platform::CUDAPinnedPlace&gt;(
                boost::get&lt;platform::CUDAPinnedPlace&gt;(place), size, type));
        }
        }
    #endif
        offset_ = 0;
    }
    return reinterpret_cast&lt;void*&gt;(reinterpret_cast&lt;uintptr_t&gt;(holder_-&gt;ptr()) +
                                    offset_);
    }

    void* Tensor::mutable_data(platform::Place place) {
    PADDLE_ENFORCE(this-&gt;holder_ != nullptr,
                    &quot;Cannot invoke mutable data if current hold nothing.&quot;);
    return mutable_data(place, holder_-&gt;type());
    }
</code></pre>
<p><strong>dims()</strong><br />
<strong>numel()</strong><br />
<strong>Resize()</strong><br />
<strong>ShareDataWith()</strong><br />
<strong>Slice()</strong><br />
<strong>place()</strong><br />
<strong>type()</strong><br />
<strong>memory_size()</strong><br />
<strong>check_memory_size()</strong><br />
<strong>layout()</strong><br />
<strong>set_layout()</strong></p>


</div>
<br /><br /> 
</div></div>


<div class="page-bottom">
  <div class="row">
  <hr />
  <div class="small-9 columns">
  <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
  <div class="small-3 columns">
  <p class="copyright text-right"><a href="#header">TOP</a></p>
  </div>
   
  </div>
</div>

        </section>
      </div>
    </div>
    



  













<script src="asset/prism.js"></script>


<style type="text/css">
figure{margin: 0.4em 0;padding: 0;}
  figcaption{text-align:center;}

/* PrismJS 1.14.0
 http://prismjs.com/download.html#themes=prism&languages=markup+css+clike+javascript */
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
    color: black;
    background: none;
    text-shadow: 0 1px white;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
    
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
    
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
    text-shadow: none;
    background:#b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
    text-shadow: none;
    background: #b3d4fc;
}

@media print {
    code[class*="language-"],
    pre[class*="language-"] {
        text-shadow: none;
    }
}

/* Code blocks */
pre[class*="language-"] {
    padding: 1em;
    margin: .5em 0;
    overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
    background: #F7F7F7;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
    padding: .1em;
    border-radius: .3em;
    white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
    color: slategray;
}

.token.punctuation {
    color: #999;
}

.namespace {
    opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
    color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
    color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
    color: #9a6e3a;
    background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
    color: #07a;
}

.token.function,
.token.class-name {
    color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
    color: #e90;
}

.token.important,
.token.bold {
    font-weight: bold;
}
.token.italic {
    font-style: italic;
}

.token.entity {
    cursor: help;
}


pre[class*="language-"].line-numbers {
    position: relative;
    padding-left: 3.8em;
    counter-reset: linenumber;
}

pre[class*="language-"].line-numbers > code {
    position: relative;
    white-space: inherit;
}

.line-numbers .line-numbers-rows {
    position: absolute;
    pointer-events: none;
    top: 0;
    font-size: 100%;
    left: -3.8em;
    width: 3em; /* works for line-numbers below 1000 lines */
    letter-spacing: -1px;
    border-right: 1px solid #999;

    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;

}

    .line-numbers-rows > span {
        pointer-events: none;
        display: block;
        counter-increment: linenumber;
    }

        .line-numbers-rows > span:before {
            content: counter(linenumber);
            color: #999;
            display: block;
            padding-right: 0.8em;
            text-align: right;
        }


</style>
    
    <script src="asset/js/foundation.min.js"></script>
    <script src="asset/js/foundation/foundation.offcanvas.js"></script>
    <script>
      $(document).foundation();

     
    </script>


  </body>
</html>
